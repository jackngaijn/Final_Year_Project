{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"â‚¬\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "    \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jack/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 221464\n"
     ]
    }
   ],
   "source": [
    "base_csv = './data/IMDB Dataset.csv'\n",
    "df = pd.read_csv(base_csv)\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stop words (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "# Preprocess the text to extract the vocabulary\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = abbreviations[text.lower()] if text.lower() in abbreviations.keys() else text\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = text.strip()\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "    text_tokens = text.split()  \n",
    "    filtered_text = [word for word in text_tokens if word not in stop_words]\n",
    "    text = ' '.join(filtered_text)\n",
    "    return text\n",
    "\n",
    "# Tokenize and build the vocabulary\n",
    "def build_vocab(dataframe, column_name):\n",
    "    word_list = []\n",
    "    for review in dataframe[column_name]:\n",
    "        processed_review = preprocess_text(review)\n",
    "        word_list.extend(processed_review.split())\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(word_list)\n",
    "    return word_counts\n",
    "\n",
    "\n",
    "vocab = build_vocab(df, 'review')\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word to index mapping: [('one', 1), ('reviewers', 2), ('mentioned', 3), ('watching', 4), ('1', 5), ('oz', 6), ('episode', 7), ('youll', 8), ('hooked', 9), ('right', 10)]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Create a word-to-index dictionary\n",
    "word_to_index = {word: i+1 for i, word in enumerate(vocab.keys())}  # Start indexing from 1\n",
    "# Convert reviews to sequences of integers\n",
    "def text_to_sequence(text, word_to_index):\n",
    "    processed_text = preprocess_text(text)\n",
    "    return [word_to_index[word] for word in processed_text.split() if word in word_to_index]\n",
    "\n",
    "# Tokenize the reviews\n",
    "df['review_sequences'] = df['review'].apply(lambda x: text_to_sequence(x, word_to_index))\n",
    "df['label_convert'] = df['sentiment'].apply(lambda x: 1 if x =='positive' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequences shape: (40000, 100)\n",
      "Padded Array: [[  626  1251  3129 ...   214 27947 10648]\n",
      " [  336   158   295 ...   919  3196  2333]\n",
      " [ 4438   332   420 ...     0     0     0]\n",
      " ...\n",
      " [ 2207     1  1167 ...     0     0     0]\n",
      " [  146   878  1069 ...     0     0     0]\n",
      " [ 3266   295  1204 ...     0     0     0]]\n",
      "Padded Array: [[   186    607  84411 ...    239   1456 106497]\n",
      " [   251   2249     88 ...   2120   3037   4395]\n",
      " [   306   4404    380 ...      0      0      0]\n",
      " ...\n",
      " [   747  63135    565 ...     81   2623   7873]\n",
      " [    79    573     23 ...      0      0      0]\n",
      " [   103   4795    259 ...      0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "# Train-test split with shuffle\n",
    "X_train, y_train, X_test, y_test = train_test_split(df['review_sequences'], df['label_convert'], test_size=0.2, random_state=42, shuffle=True)\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = 100  # Maximum sequence length\n",
    "x_train_pad = pad_sequences(X_train, maxlen=max_length, padding='post', truncating='post')\n",
    "y_train_pad = pad_sequences(y_train, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "print(f\"Padded sequences shape: {x_train_pad.shape}\")\n",
    "print(\"Padded Array:\", x_train_pad)\n",
    "print(\"Padded Array:\", y_train_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  626,  1251,  3129, ...,   214, 27947, 10648],\n",
       "       [  336,   158,   295, ...,   919,  3196,  2333],\n",
       "       [ 4438,   332,   420, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [ 2207,     1,  1167, ...,     0,     0,     0],\n",
       "       [  146,   878,  1069, ...,     0,     0,     0],\n",
       "       [ 3266,   295,  1204, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.7840\n",
      "Epoch 2/10, Loss: 0.6433\n",
      "Epoch 3/10, Loss: 0.6079\n",
      "Epoch 4/10, Loss: 0.6003\n",
      "Epoch 5/10, Loss: 0.5962\n",
      "Epoch 6/10, Loss: 0.5924\n",
      "Epoch 7/10, Loss: 0.5886\n",
      "Epoch 8/10, Loss: 0.5848\n",
      "Epoch 9/10, Loss: 0.5811\n",
      "Epoch 10/10, Loss: 0.5774\n",
      "\n",
      "--- Evaluation on Sample Input ---\n",
      "Text: 'i love this movie'\n",
      "Predicted class: 1\n",
      "Logits: [-0.32629627  0.03500685]\n",
      "Saliency for each word (higher value means more influence):\n",
      "  i: 0.0000\n",
      "  love: 0.0000\n",
      "  this: 0.0001\n",
      "  movie: 0.0132\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Define Vocabulary and Utility Functions\n",
    "# ------------------------------\n",
    "\n",
    "# Define a simple vocabulary mapping.\n",
    "vocab = {\n",
    "    \"i\": 0,\n",
    "    \"love\": 1,\n",
    "    \"hate\": 2,\n",
    "    \"this\": 3,\n",
    "    \"movie\": 4,\n",
    "    \"film\": 5,\n",
    "    \"is\": 6,\n",
    "    \"amazing\": 7,\n",
    "    \"awful\": 8\n",
    "}\n",
    "\n",
    "def text_to_indices(text, vocab):\n",
    "    \"\"\"\n",
    "    Converts a text string to a list of indices based on the given vocabulary.\n",
    "    \"\"\"\n",
    "    return [vocab[word] for word in text.split() if word in vocab]\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Define the Supervised HMM Classifier Model\n",
    "# ------------------------------\n",
    "\n",
    "class SupervisedHMMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_states, num_classes):\n",
    "        \"\"\"\n",
    "        vocab_size: the size of the vocabulary.\n",
    "        embed_dim: dimensionality of word embeddings.\n",
    "        num_states: number of hidden states in the HMM.\n",
    "        num_classes: number of classes to predict.\n",
    "        \"\"\"\n",
    "        super(SupervisedHMMClassifier, self).__init__()\n",
    "        # Embedding layer to convert word indices into embeddings.\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # HMM parameters:\n",
    "        # Initial state distribution (pi) for the hidden states.\n",
    "        self.pi = nn.Parameter(torch.rand(num_states))\n",
    "        # Transition matrix A between states.\n",
    "        self.A = nn.Parameter(torch.rand(num_states, num_states))\n",
    "        # For each state, define a linear layer that computes a scalar emission score\n",
    "        # from the word embedding. We exponentiate these scores to ensure positive emissions.\n",
    "        self.emission_layers = nn.ModuleList(\n",
    "            [nn.Linear(embed_dim, 1) for _ in range(num_states)]\n",
    "        )\n",
    "        \n",
    "        # A simple classifier: maps the final latent state (vector of size num_states)\n",
    "        # to class logits.\n",
    "        self.classifier = nn.Linear(num_states, num_classes)\n",
    "    \n",
    "    def forward(self, x, embeddings=None, return_embeddings=False):\n",
    "        \"\"\"\n",
    "        x: tensor of word indices with shape (T,), where T is the sentence length.\n",
    "        embeddings: (optional) precomputed embeddings for x. If not provided, they are computed.\n",
    "        return_embeddings: if True, also return intermediate values for saliency analysis.\n",
    "        \n",
    "        Returns:\n",
    "          logits: class scores (before softmax).\n",
    "          final_alpha: final latent state representation from the HMM (normalized).\n",
    "          (optionally) embeddings, the full list of alphas (for each time step), and emissions.\n",
    "        \"\"\"\n",
    "        if embeddings is None:\n",
    "            embeddings = self.embedding(x)  # shape: (T, embed_dim)\n",
    "        \n",
    "        T = embeddings.size(0)  # Sentence length\n",
    "        num_states = self.pi.size(0)\n",
    "        \n",
    "        # Compute emission probabilities for each state and time step.\n",
    "        emission_list = []\n",
    "        for state in range(num_states):\n",
    "            out = self.emission_layers[state](embeddings).squeeze(-1)  # shape: (T,)\n",
    "            emission_list.append(torch.exp(out))  # Ensure positive emissions.\n",
    "        # Stack into shape (T, num_states)\n",
    "        emissions = torch.stack(emission_list, dim=1)\n",
    "        \n",
    "        # --- Forward Algorithm ---\n",
    "        alpha_list = []\n",
    "        # Initialization (t=0)\n",
    "        alpha_t = self.pi * emissions[0]  # shape: (num_states,)\n",
    "        alpha_list.append(alpha_t)\n",
    "        \n",
    "        # Recursion for t = 1, â€¦, T-1:\n",
    "        for t in range(1, T):\n",
    "            alpha_prev = alpha_list[-1]\n",
    "            # For each state j:\n",
    "            # alpha[t, j] = emissions[t, j] * sum_i (alpha[t-1, i] * A[i, j])\n",
    "            alpha_t = emissions[t] * torch.matmul(alpha_prev, self.A)\n",
    "            alpha_list.append(alpha_t)\n",
    "        \n",
    "        # Stack along time steps: shape (T, num_states)\n",
    "        alpha = torch.stack(alpha_list, dim=0)\n",
    "        # Normalize final alpha\n",
    "        final_alpha = alpha[-1] / (alpha[-1].sum() + 1e-8)\n",
    "        \n",
    "        # --- Classification ---\n",
    "        logits = self.classifier(final_alpha)  # shape: (num_classes,)\n",
    "        \n",
    "        if return_embeddings:\n",
    "            return logits, final_alpha, embeddings, alpha, emissions\n",
    "        else:\n",
    "            return logits, final_alpha\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Define a Saliency Computation Function\n",
    "# ------------------------------\n",
    "\n",
    "def compute_saliency(text, model):\n",
    "    \"\"\"\n",
    "    Computes word-level saliency for a given text using gradient norms.\n",
    "    \n",
    "    Returns:\n",
    "      saliency: a tensor of shape (T,) with a saliency value for each word.\n",
    "      logits: the class logits computed by the model.\n",
    "      predicted_class: the index of the predicted class.\n",
    "    \"\"\"\n",
    "    indices = text_to_indices(text, vocab)\n",
    "    x = torch.tensor(indices, dtype=torch.long)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Get embeddings and enable gradient tracking.\n",
    "    embeddings = model.embedding(x)  # shape: (T, embed_dim)\n",
    "    embeddings.requires_grad_(True)\n",
    "    embeddings.retain_grad()\n",
    "\n",
    "    # Forward pass (with intermediate values for saliency analysis)\n",
    "    logits, final_alpha, used_embeddings, alpha, emissions = model(x, embeddings=embeddings, return_embeddings=True)\n",
    "    \n",
    "    # Select predicted class as the one with maximum logit.\n",
    "    predicted_class = torch.argmax(logits)\n",
    "    \n",
    "    # Use the logit for the predicted class as the target.\n",
    "    target_logit = logits[predicted_class]\n",
    "    \n",
    "    # Backpropagate to compute gradients with respect to embeddings.\n",
    "    target_logit.backward()\n",
    "    \n",
    "    # Compute the L2 norm of the gradients for each word.\n",
    "    saliency = embeddings.grad.norm(dim=1)\n",
    "    return saliency, logits, predicted_class.item()\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Create Fake Data and Train the Model\n",
    "# ------------------------------\n",
    "\n",
    "# Define a small fake training dataset.\n",
    "# Each training instance is a tuple (text, label)\n",
    "# For this example, label 1 will denote a positive sentiment and label 0 a negative sentiment.\n",
    "train_data = [\n",
    "    (\"i love this movie\", 1),\n",
    "    (\"i hate this movie\", 0),\n",
    "    (\"this film is amazing\", 1),\n",
    "    (\"this movie is awful\", 0),\n",
    "]\n",
    "\n",
    "# Hyperparameters.\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 100\n",
    "num_states = 2\n",
    "num_classes = 2\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Instantiate the model and move it to device.\n",
    "model = SupervisedHMMClassifier(vocab_size, embed_dim, num_states, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop.\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for text, label in train_data:\n",
    "        optimizer.zero_grad()  # Reset gradients for each example.\n",
    "        \n",
    "        # Convert text to indices tensor.\n",
    "        indices = text_to_indices(text, vocab)\n",
    "        x = torch.tensor(indices, dtype=torch.long)\n",
    "        target = torch.tensor([label], dtype=torch.long)\n",
    "        \n",
    "        # Forward pass.\n",
    "        logits, final_alpha = model(x)\n",
    "        # logits shape: (num_classes,) => unsqueeze to (1, num_classes)\n",
    "        loss = loss_fn(logits.unsqueeze(0), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_data):.4f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Evaluate and Compute Saliency for a Sample Input\n",
    "# ------------------------------\n",
    "\n",
    "model.eval()\n",
    "sample_text = \"i love this movie\"\n",
    "saliency, logits, pred_class = compute_saliency(sample_text, model)\n",
    "\n",
    "print(\"\\n--- Evaluation on Sample Input ---\")\n",
    "print(f\"Text: '{sample_text}'\")\n",
    "print(f\"Predicted class: {pred_class}\")\n",
    "print(f\"Logits: {logits.detach().numpy()}\")\n",
    "print(\"Saliency for each word (higher value means more influence):\")\n",
    "for word, sal in zip(sample_text.split(), saliency):\n",
    "    print(f\"  {word}: {sal.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
