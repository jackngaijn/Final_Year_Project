{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.optimization because of the following error (look up to see its traceback):\ncannot import name 'is_torch_hpu_available' from 'transformers.utils' (/opt/anaconda3/envs/FYP/lib/python3.10/site-packages/transformers/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/FYP/lib/python3.10/site-packages/transformers/utils/import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1817\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   1818\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FYP/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FYP/lib/python3.10/site-packages/transformers/optimization.py:26\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39mtrainer_pt_utils\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m LayerWiseDummyOptimizer, LayerWiseDummyScheduler\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39mtrainer_utils\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m SchedulerType\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m logging\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FYP/lib/python3.10/site-packages/transformers/trainer_utils.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnumpy\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnp\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     ExplicitEnum,\n\u001b[1;32m     33\u001b[0m     is_psutil_available,\n\u001b[1;32m     34\u001b[0m     is_tf_available,\n\u001b[1;32m     35\u001b[0m     is_torch_available,\n\u001b[1;32m     36\u001b[0m     is_torch_cuda_available,\n\u001b[1;32m     37\u001b[0m     is_torch_hpu_available,\n\u001b[1;32m     38\u001b[0m     is_torch_mlu_available,\n\u001b[1;32m     39\u001b[0m     is_torch_mps_available,\n\u001b[1;32m     40\u001b[0m     is_torch_musa_available,\n\u001b[1;32m     41\u001b[0m     is_torch_npu_available,\n\u001b[1;32m     42\u001b[0m     is_torch_xla_available,\n\u001b[1;32m     43\u001b[0m     is_torch_xpu_available,\n\u001b[1;32m     44\u001b[0m     requires_backends,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m is_torch_available():\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'is_torch_hpu_available' from 'transformers.utils' (/opt/anaconda3/envs/FYP/lib/python3.10/site-packages/transformers/utils/__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtransformers\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdatasets\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m accuracy_score\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FYP/lib/python3.10/site-packages/transformers/utils/import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     value \u001b[39m=\u001b[39m Placeholder\n\u001b[1;32m   1804\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m-> 1805\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[1;32m   1806\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1807\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/FYP/lib/python3.10/site-packages/transformers/utils/import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1817\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m   1818\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1819\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1820\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1821\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1822\u001b[0m     ) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.optimization because of the following error (look up to see its traceback):\ncannot import name 'is_torch_hpu_available' from 'transformers.utils' (/opt/anaconda3/envs/FYP/lib/python3.10/site-packages/transformers/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Step 1: Import required libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 2: Load tokenizer and pre-trained DistilBERT model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Binary classification\n",
    "\n",
    "# Step 3: Load dataset (IMDB Sentiment Analysis Dataset)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Step 4: Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Step 5: Convert the dataset to PyTorch format\n",
    "def format_dataset(dataset):\n",
    "    return dataset.with_format(\"torch\")\n",
    "\n",
    "train_dataset = format_dataset(tokenized_datasets[\"train\"])\n",
    "test_dataset = format_dataset(tokenized_datasets[\"test\"])\n",
    "\n",
    "# Step 6: Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=64)\n",
    "\n",
    "# Step 7: Define optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = len(train_dataloader) * 3  # 3 epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Step 8: Set up device (GPU/CPU)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Step 9: Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # Send batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(f\"Training loss: {loss.item()}\")\n",
    "\n",
    "# Step 10: Evaluation\n",
    "model.eval()  # Set model to evaluation mode\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Step 11: Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_distilbert\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_distilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
