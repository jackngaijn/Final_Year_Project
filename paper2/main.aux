\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nyt/global//global/global}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{bengio1994learning}
\abx@aux@segm{0}{0}{bengio1994learning}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{hochreiter1997lstm}
\abx@aux@segm{0}{0}{hochreiter1997lstm}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{cho2014properties}
\abx@aux@segm{0}{0}{cho2014properties}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\abx@aux@page{1}{3}
\abx@aux@page{2}{3}
\abx@aux@page{3}{3}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Literature Review}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Evolution of Deep Learning and Recurrent Neural Networks}{3}{subsection.2.1}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{werbos1990bptt}
\abx@aux@segm{0}{0}{werbos1990bptt}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{rumelhart1986backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986backpropagation}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{werbos1990bptt}
\abx@aux@segm{0}{0}{werbos1990bptt}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Literature Review}{4}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Backpropagation Through Time}{4}{subsubsection.2.2.1}\protected@file@percent }
\abx@aux@page{4}{4}
\abx@aux@page{5}{4}
\abx@aux@page{6}{4}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Unfolded RNN}}{4}{figure.1}\protected@file@percent }
\gdef \LT@i {\LT@entry 
    {1}{64.36809pt}\LT@entry 
    {1}{253.32529pt}\LT@entry 
    {1}{180.16806pt}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{Unfolded RNN}}{7}{table.1}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{wikipedia2023bptt}
\abx@aux@segm{0}{0}{wikipedia2023bptt}
\abx@aux@page{7}{8}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Backpropagation Through Time (BPTT)}}{8}{algorithm.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Activation Function}{8}{subsubsection.2.2.2}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{bengio1994learning}
\abx@aux@segm{0}{0}{bengio1994learning}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{hochreiter1997lstm}
\abx@aux@segm{0}{0}{hochreiter1997lstm}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{hochreiter1997lstm}
\abx@aux@segm{0}{0}{hochreiter1997lstm}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Gradient vanishing and gradient exploring}{10}{subsubsection.2.2.3}\protected@file@percent }
\abx@aux@page{8}{10}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Long short-term memory (LSTM)}{10}{subsubsection.2.2.4}\protected@file@percent }
\abx@aux@page{9}{10}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{hochreiter1997lstm}
\abx@aux@segm{0}{0}{hochreiter1997lstm}
\abx@aux@page{10}{11}
\abx@aux@page{11}{11}
\gdef \LT@ii {\LT@entry 
    {1}{64.36809pt}\LT@entry 
    {1}{244.38232pt}\LT@entry 
    {1}{77.8466pt}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{Unfolded RNN}}{12}{table.2}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{cho2014properties}
\abx@aux@segm{0}{0}{cho2014properties}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5}Gated Recurrent Unit (GRU)}{13}{subsubsection.2.2.5}\protected@file@percent }
\abx@aux@page{12}{13}
\gdef \LT@iii {\LT@entry 
    {1}{64.36809pt}\LT@entry 
    {1}{244.38232pt}\LT@entry 
    {1}{77.8466pt}}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{bengio2009learning}
\abx@aux@segm{0}{0}{bengio2009learning}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{le2010deep}
\abx@aux@segm{0}{0}{le2010deep}
\abx@aux@cite{0}{delalleau2011shallow}
\abx@aux@segm{0}{0}{delalleau2011shallow}
\abx@aux@cite{0}{pascanu2013on}
\abx@aux@segm{0}{0}{pascanu2013on}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.6}Deep recurrent neural networks (DRNNs)}{15}{subsubsection.2.2.6}\protected@file@percent }
\abx@aux@page{13}{15}
\abx@aux@page{14}{15}
\abx@aux@page{15}{15}
\abx@aux@page{16}{15}
\gdef \LT@iv {\LT@entry 
    {1}{64.36809pt}\LT@entry 
    {1}{281.65555pt}\LT@entry 
    {1}{160.3602pt}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.7}RNN encoder decoder model}{16}{subsubsection.2.2.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{Encoder}}{16}{table.4}\protected@file@percent }
\gdef \LT@v {\LT@entry 
    {1}{64.36809pt}\LT@entry 
    {1}{195.75089pt}\LT@entry 
    {1}{203.54005pt}}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{bahdanau2016neuralmachinetranslationjointly}
\abx@aux@segm{0}{0}{bahdanau2016neuralmachinetranslationjointly}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{luong2015effectiveapproachesattentionbasedneural}
\abx@aux@segm{0}{0}{luong2015effectiveapproachesattentionbasedneural}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{vaswani2017attentionneed}
\abx@aux@segm{0}{0}{vaswani2017attentionneed}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{luong2015effectiveapproachesattentionbasedneural}
\abx@aux@segm{0}{0}{luong2015effectiveapproachesattentionbasedneural}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{roberts2004general}
\abx@aux@segm{0}{0}{roberts2004general}
\abx@aux@cite{0}{rabiner1986anintroduction}
\abx@aux@segm{0}{0}{rabiner1986anintroduction}
\@writefile{lot}{\contentsline {table}{\numberline {5}{Decoder}}{17}{table.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.8}Attention Mechanism}{17}{subsubsection.2.2.8}\protected@file@percent }
\abx@aux@page{17}{17}
\abx@aux@page{18}{17}
\abx@aux@page{19}{17}
\abx@aux@page{20}{17}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{rabiner1989atutorial}
\abx@aux@segm{0}{0}{rabiner1989atutorial}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.9}Hidden Markov Model}{18}{subsubsection.2.2.9}\protected@file@percent }
\abx@aux@page{21}{18}
\abx@aux@page{22}{18}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{rabiner1993fundamentals}
\abx@aux@segm{0}{0}{rabiner1993fundamentals}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{sengupta2023hybrid}
\abx@aux@segm{0}{0}{sengupta2023hybrid}
\abx@aux@page{23}{19}
\abx@aux@page{24}{19}
\abx@aux@page{25}{19}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{rabiner1993fundamentals}
\abx@aux@segm{0}{0}{rabiner1993fundamentals}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{mikolov2013efficient}
\abx@aux@segm{0}{0}{mikolov2013efficient}
\abx@aux@page{26}{20}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.10}Word representation}{20}{subsubsection.2.2.10}\protected@file@percent }
\abx@aux@page{27}{20}
\@writefile{toc}{\contentsline {section}{\numberline {3}Project Goals and Objectives}{21}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Project Goals}{21}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Objectives}{21}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Implement Existing RNN Architectures}{21}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Apply the Models on a Benchmark NLP Dataset}{21}{subsubsection.3.2.2}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{maas-EtAl:2011:ACL-HLT2011}
\abx@aux@segm{0}{0}{maas-EtAl:2011:ACL-HLT2011}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Compare Model Performances}{22}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Analyze the Impact of Architectural Differences}{22}{subsubsection.3.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Research Plan / Methodology}{22}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Literature Review}{22}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Data Collection and Preprocessing}{22}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Data Collection}{22}{subsubsection.4.2.1}\protected@file@percent }
\abx@aux@page{28}{22}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Data Preprocessing}{22}{subsubsection.4.2.2}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{mikolov2013efficient}
\abx@aux@segm{0}{0}{mikolov2013efficient}
\abx@aux@page{29}{23}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Word to Index Mapping Table}}{23}{table.6}\protected@file@percent }
\newlabel{tab:word_index}{{6}{23}{Word to Index Mapping Table}{table.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Model Architecture}{23}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Word to Index Matching with Lookup Table Embeddings}}{24}{table.7}\protected@file@percent }
\newlabel{tab:word_lookup}{{7}{24}{Word to Index Matching with Lookup Table Embeddings}{table.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Model 1 (Vanilla RNN)}{24}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Model 2 (Long short-term memory)}{25}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 1: Initialize Hidden and Cell States}{25}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 2: Process Each Input Vector}{25}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Compute Forget Gate}{26}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 4: Compute Input Gate}{26}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 5: Compute Cell Candidate}{26}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 6: Update Cell State}{26}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 7: Compute Output Gate}{26}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 8: Compute Hidden State}{27}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 9: Use Final Hidden State for Output}{27}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Model 3 (Gated recurrent unit)}{27}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 1: Initialize Hidden State}{27}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 2: Process Each Input Vector}{27}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Compute Reset Gate}{28}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 4: Compute Update Gate}{28}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 5: Compute Candidate Hidden State}{28}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 6: Update Hidden State}{28}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 7: Use Final Hidden State for Output}{29}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Model 4 (Vanilla RNN with 2 layer)}{29}{subsubsection.4.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.5}Model 5 (Gated recurrent unit with 2 layer)}{29}{subsubsection.4.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.6}Model 6 (Long short-term memory with 2 layer)}{29}{subsubsection.4.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.7}Model 7 (RNN Encoder-Decoder)}{29}{subsubsection.4.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.8}Model 8 (RNN Encoder-Decoder with attention mechanism)}{29}{subsubsection.4.3.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Training, Hyperparameter Tuning, and Evaluation}{30}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Model Hyperparameters and Setup}{30}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Training Loop Overview}{31}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Pseudocode Description}{32}{subsubsection.4.4.3}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}Training Loop Pseudocode}{32}{lstlisting.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results and Analysis}{34}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Results}{34}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Model 1 (Vanilla RNN)}{34}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces RNN with 1 layer performance on IMDB dataset}}{34}{figure.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Model 4 (Vanilla RNN with 2 layer)}{34}{subsubsection.5.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces RNN with 2 layer performance on IMDB dataset}}{35}{figure.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Model 3 (Gated recurrent unit)}{35}{subsubsection.5.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces GRU with 1 layer performance on IMDB dataset}}{35}{figure.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.4}Model 5 (Gated recurrent unit with 2 layer)}{35}{subsubsection.5.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces GRU with 2 layer performance on IMDB dataset}}{36}{figure.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.5}Model 2 (Long short-term memory)}{36}{subsubsection.5.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces LSTM with 1 layer performance on IMDB dataset}}{36}{figure.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.6}Model 6 (Long short-term memory with 2 layer)}{36}{subsubsection.5.1.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces LSTM with 2 layer performance on IMDB dataset}}{37}{figure.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.7}Model 7 (RNN encoder-decoder)}{38}{subsubsection.5.1.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces LSTM with 2 layer performance on IMDB dataset}}{38}{figure.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.8}Model 8 (RNN encoder-decoder with attention mechanism)}{39}{subsubsection.5.1.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces LSTM with 2 layer performance on IMDB dataset}}{39}{figure.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.9}Word Importancy}{40}{subsubsection.5.1.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Heatmap of hidden state}}{40}{figure.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces t-SNE}}{41}{figure.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion and Conclusion}{42}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Results and Analysis}{42}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Model Comparisons}{42}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Overfitting Issues}{42}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Deep Recurrent Neural Networks (DRNN)}{42}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Attention Mechanism}{42}{subsection.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Gradient Problem}{43}{subsection.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1}Vanilla RNN (Red line)}{43}{subsubsection.6.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.2}LSTM (Blue line)}{43}{subsubsection.6.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.3}GRU (Green line)}{44}{subsubsection.6.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion and Conclusion}{44}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Discussion}{44}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Conclusion}{44}{subsection.7.2}\protected@file@percent }
\abx@aux@page{30}{46}
\abx@aux@page{31}{46}
\abx@aux@page{32}{46}
\abx@aux@page{33}{46}
\abx@aux@page{34}{46}
\abx@aux@page{35}{46}
\abx@aux@page{36}{46}
\abx@aux@page{37}{46}
\abx@aux@page{38}{46}
\abx@aux@page{39}{46}
\abx@aux@page{40}{46}
\abx@aux@page{41}{46}
\abx@aux@page{42}{46}
\abx@aux@page{43}{46}
\abx@aux@page{44}{46}
\abx@aux@page{45}{46}
\abx@aux@page{46}{46}
\abx@aux@page{47}{46}
\abx@aux@page{48}{47}
\abx@aux@page{49}{47}
\abx@aux@read@bbl@mdfivesum{548ACC8613C623C58CF56752C3645B22}
\abx@aux@defaultrefcontext{0}{bahdanau2016neuralmachinetranslationjointly}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{bengio2009learning}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{bengio1994learning}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{cho2014properties}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{delalleau2011shallow}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{hochreiter1997lstm}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{le2010deep}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{luong2015effectiveapproachesattentionbasedneural}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{maas-EtAl:2011:ACL-HLT2011}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{mikolov2013efficient}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{pascanu2013on}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{rabiner1986anintroduction}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{rabiner1989atutorial}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{rabiner1993fundamentals}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{roberts2004general}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{rumelhart1986backpropagation}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{sengupta2023hybrid}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{vaswani2017attentionneed}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{werbos1990bptt}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{wikipedia2023bptt}{nyt/global//global/global}
\gdef \@abspage@last{47}
