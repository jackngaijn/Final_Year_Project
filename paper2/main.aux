\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nyt/global//global/global}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{bengio1994learning}
\abx@aux@segm{0}{0}{bengio1994learning}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{hochreiter1997lstm}
\abx@aux@segm{0}{0}{hochreiter1997lstm}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{cho2014properties}
\abx@aux@segm{0}{0}{cho2014properties}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Literature Review}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Evolution of Deep Learning and Recurrent Neural Networks}{3}{subsection.2.1}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{werbos1990bptt}
\abx@aux@segm{0}{0}{werbos1990bptt}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{rumelhart1986backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986backpropagation}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{werbos1990bptt}
\abx@aux@segm{0}{0}{werbos1990bptt}
\gdef \LT@i {\LT@entry 
    {1}{64.36809pt}\LT@entry 
    {1}{253.32529pt}\LT@entry 
    {1}{180.16806pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Literature Review}{4}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Backpropagation Through Time}{4}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{Unfolded RNN}}{4}{table.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Unfolded RNN}}{5}{figure.1}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{wikipedia2023bptt}
\abx@aux@segm{0}{0}{wikipedia2023bptt}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Backpropagation Through Time (BPTT)}}{8}{algorithm.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Activation Function}{8}{subsubsection.2.2.2}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{bengio1994learning}
\abx@aux@segm{0}{0}{bengio1994learning}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{hochreiter1997lstm}
\abx@aux@segm{0}{0}{hochreiter1997lstm}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{hochreiter1997lstm}
\abx@aux@segm{0}{0}{hochreiter1997lstm}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Gradient vanishing and gradient exploring}{10}{subsubsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Long short-term memory (LSTM)}{10}{subsubsection.2.2.4}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{hochreiter1997lstm}
\abx@aux@segm{0}{0}{hochreiter1997lstm}
\gdef \LT@ii {\LT@entry 
    {1}{64.36809pt}\LT@entry 
    {1}{244.38232pt}\LT@entry 
    {1}{77.8466pt}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{Unfolded RNN}}{12}{table.2}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{cho2014properties}
\abx@aux@segm{0}{0}{cho2014properties}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5}Gated Recurrent Unit (GRU)}{13}{subsubsection.2.2.5}\protected@file@percent }
\gdef \LT@iii {\LT@entry 
    {1}{64.36809pt}\LT@entry 
    {1}{244.38232pt}\LT@entry 
    {1}{77.8466pt}}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{bengio2009learning}
\abx@aux@segm{0}{0}{bengio2009learning}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{le2010deep}
\abx@aux@segm{0}{0}{le2010deep}
\abx@aux@cite{0}{delalleau2011shallow}
\abx@aux@segm{0}{0}{delalleau2011shallow}
\abx@aux@cite{0}{pascanu2013on}
\abx@aux@segm{0}{0}{pascanu2013on}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.6}Deep recurrent neural networks (DRNNs)}{15}{subsubsection.2.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.7}RNN encoder decoder model}{16}{subsubsection.2.2.7}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{vaswani2023attentionneed}
\abx@aux@segm{0}{0}{vaswani2023attentionneed}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{bahdanau2016neuralmachinetranslationjointly}
\abx@aux@segm{0}{0}{bahdanau2016neuralmachinetranslationjointly}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{bahdanau2016neuralmachinetranslationjointly}
\abx@aux@segm{0}{0}{bahdanau2016neuralmachinetranslationjointly}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{luong2015effectiveapproachesattentionbasedneural}
\abx@aux@segm{0}{0}{luong2015effectiveapproachesattentionbasedneural}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{roberts2004general}
\abx@aux@segm{0}{0}{roberts2004general}
\abx@aux@cite{0}{rabiner1986anintroduction}
\abx@aux@segm{0}{0}{rabiner1986anintroduction}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.8}Attention Mechanism}{17}{subsubsection.2.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.9}Hidden Markov Model}{17}{subsubsection.2.2.9}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{rabiner1989atutorial}
\abx@aux@segm{0}{0}{rabiner1989atutorial}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{rabiner1993fundamentals}
\abx@aux@segm{0}{0}{rabiner1993fundamentals}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{sengupta2023hybrid}
\abx@aux@segm{0}{0}{sengupta2023hybrid}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{rabiner1993fundamentals}
\abx@aux@segm{0}{0}{rabiner1993fundamentals}
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{mikolov2013efficient}
\abx@aux@segm{0}{0}{mikolov2013efficient}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.10}Word representation}{20}{subsubsection.2.2.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Project Goals and Objectives}{20}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Project Goals}{20}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Objectives}{21}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Implement Existing RNN Architectures}{21}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Apply the Models on a Benchmark NLP Dataset}{21}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Compare Model Performances}{21}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Analyze the Impact of Architectural Differences}{21}{subsubsection.3.2.4}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{maas-EtAl:2011:ACL-HLT2011}
\abx@aux@segm{0}{0}{maas-EtAl:2011:ACL-HLT2011}
\@writefile{toc}{\contentsline {section}{\numberline {4}Research Plan / Methodology}{22}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Literature Review}{22}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Data Collection and Preprocessing}{22}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Data Collection}{22}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Data Preprocessing}{22}{subsubsection.4.2.2}\protected@file@percent }
\abx@aux@refcontext{nyt/apasortcite//global/global}
\abx@aux@cite{0}{mikolov2013efficient}
\abx@aux@segm{0}{0}{mikolov2013efficient}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Word to Index Mapping Table}}{23}{table.4}\protected@file@percent }
\newlabel{tab:word_index}{{4}{23}{Word to Index Mapping Table}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Model Architecture}{23}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Word to Index Matching with Lookup Table Embeddings}}{24}{table.5}\protected@file@percent }
\newlabel{tab:word_lookup}{{5}{24}{Word to Index Matching with Lookup Table Embeddings}{table.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Model 1 (Vanilla RNN)}{24}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Model 2 (Long short-term memory)}{25}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 1: Initialize Hidden and Cell States}{25}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 2: Process Each Input Vector}{25}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Compute Forget Gate}{25}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 4: Compute Input Gate}{26}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 5: Compute Cell Candidate}{26}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 6: Update Cell State}{26}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 7: Compute Output Gate}{26}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 8: Compute Hidden State}{26}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 9: Use Final Hidden State for Output}{27}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Model 3 (Gated recurrent unit)}{27}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 1: Initialize Hidden State}{27}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 2: Process Each Input Vector}{27}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Compute Reset Gate}{28}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 4: Compute Update Gate}{28}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 5: Compute Candidate Hidden State}{28}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 6: Update Hidden State}{28}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 7: Use Final Hidden State for Output}{29}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Model 4 (Vanilla RNN with 2 layer)}{29}{subsubsection.4.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.5}Model 5 (GRU with 2 layer)}{29}{subsubsection.4.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.6}Model 6 (LSTM RNN with 2 layer)}{29}{subsubsection.4.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Training, Hyperparameter Tuning, and Evaluation}{30}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Model Hyperparameters and Setup}{30}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Training Loop Overview}{30}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Pseudocode Description}{31}{subsubsection.4.4.3}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}Training Loop Pseudocode}{31}{lstlisting.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results and Analysis}{33}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Results}{33}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Vanilla RNN (1 layer)}{33}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces RNN with 1 layer performance on IMDB dataset}}{33}{figure.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Vanilla RNN (2 layer)}{33}{subsubsection.5.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces RNN with 2 layer performance on IMDB dataset}}{34}{figure.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}GRU (1 layer)}{34}{subsubsection.5.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces GRU with 1 layer performance on IMDB dataset}}{34}{figure.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.4}GRU (2 layer)}{34}{subsubsection.5.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces GRU with 2 layer performance on IMDB dataset}}{35}{figure.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.5}LSTM (1 layer)}{35}{subsubsection.5.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces LSTM with 1 layer performance on IMDB dataset}}{35}{figure.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.6}LSTM (2 layer)}{35}{subsubsection.5.1.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces LSTM with 2 layer performance on IMDB dataset}}{36}{figure.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Analysis}{37}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Learning Behavior}{37}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Vanilla RNN (Red line)}{38}{subsubsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}LSTM (Blue line)}{38}{subsubsection.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}GRU (Green line)}{39}{subsubsection.6.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion and Conclusion}{40}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Discussion}{40}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Conclusion}{40}{subsection.7.2}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{D41D8CD98F00B204E9800998ECF8427E}
\abx@aux@read@bblrerun
\gdef \@abspage@last{40}
