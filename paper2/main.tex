\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc} % For UTF-8 encoding
\usepackage{amsmath, amssymb} % For mathematical symbols
\usepackage{graphicx} % For including images
\usepackage{hyperref} % For clickable links
\usepackage{geometry} % For page margins
\usepackage{booktabs} % For nice tables
\usepackage{lipsum} % For dummy text
\usepackage{longtable}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings} % For python code 
\usepackage{tikz} 
\usetikzlibrary{positioning, arrows.meta}
\usetikzlibrary{shapes.geometric, arrows, positioning}

\tikzstyle{block} = [rectangle, draw, text centered, rounded corners, minimum height=2em, minimum width=4em]
\tikzstyle{arrow} = [thick,->,>=stealth]

\usepackage[style=apa, sorting=nyt, backend=biber]{biblatex} % Use biber as the backend
\addbibresource{references.bib}

\setlength{\parindent}{0em}
% Page layout
\geometry{margin=1in}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={LaTeX Template},
    pdfpagemode=FullScreen,
}

% ######################## Title  Page ####################################
\title{Recurrent Deep Learning Models and its applications}
\author{Ngai Ho Wang}

\date{\today}

\begin{document}

\maketitle
% ######################## Title  Page ####################################

% ######################## table of content ###############################
\tableofcontents % Generate a table of contents                        
% ######################## table of content ###############################


% ######################## Introduction ####################################
\section{Introduction}

With the rise of the Generative Artificial Intelligence, the development of AI has already made remarkable strides in processing sequential data. In understanding and producing sequential data. It has applications ranging from Natural Language Processing (NLP) to music composition to video generation. Especially NLP, has emerged as a pivotal field in artificial intelligence, enable machines to understand, interpret and generate in human readable format. Some famous Artificial Intellience assistence for example, Siri, Alexa and Bixby have shown the possibility. Everyone can communicate with those machines, which make the reasonable response back to user.\\[1ex]
Recurrent Neural Networks (RNNs) have been a foundational architecture in this domain, Unlike the traditional Artificial Neural Network, RNNs do not treat each input independently, RNNs handle each input by considering the information from previous inputs. Conceptually this architecture able to retain the information. Thus, this architecture is suitable for handling sequential data. Unfortunately, early RNNs had limitation in training of networks over long sequence. vanishing and exploding gradient problems significantly affect the training process of RNN \parencite{bengio1994learning}. Eliminating many practical applications of RNNs. After that, \parencite{hochreiter1997lstm} introduced Long Short-Term Memory (LSTM) networks and are responsible for the breakthrough in how to solve these challenges. Specificized gating mechanisms were introduced in LSTMs to regulate the flow of the information, minimize the vanishing gradient problem and learn the long-term dependencies. This advanced made RNNs much more performant on tasks like a language modeling, machine translation and speech recognition tasks.\\[1ex]
Further improvements were achieved with Gated Recurrent Units (GRUs) by \parencite{cho2014properties} which diminished the LSTM architecture's complexity, but still provided the same performance. GRUs performed comparably but used fewer parameters, making it computationally and more tractably trainable.\\[1ex]
Since the craze of AI has been revived by generative AI, natural language processing to time series prediction and speech recognition have once again aroused people's interest in RNN. This report aims to:
\begin{itemize}
    \item Explore the theoretical foundations of recurrent deep learning models.
    \item Investigate their diverse applications in solving sequential data tasks.
    \item Analyze their performance, strengths, and inherent limitations.
\end{itemize}
% ######################## Introduction ####################################

% ######################## Background and Literature Review #########################
% Background and Literature Review 
\section{Background and Literature Review}
% Evolution of Deep Learning and Recurrent Neural Networks 
\subsection{Evolution of Deep Learning and Recurrent Neural Networks}
In the past few decades, thank to the rapidly development of technology, the computing resource has a incredible increase. Thus, substantially deep learning architecture have improved, from simple architectures, which only able to capture simple information from data to sophisticated models that are able to learn complex, abstract representations. This was before the early neural networks like perceptrons and multilayer perceptrons (MLPs) laid the footwork of neural computation that first came in the picture, but were burdened by the lack of ability to model sequential dependencies. This however imposed a limit on the feed forward paradigm, which prompted the development of recurrent neural networks (RNNs) that extend the old stalactite of feed forward paradigm with cyclic connections. Through these connection, RNNs are capable to keep a hidden state that represents information over time steps thereby effectively capture temporal dynamics. RNNs have been a decisive step in the evolution of deep learning, as they are able to do tasks that require memory of previous events, including problems of natural language processing and time series modeling. Despite that, early RNNs models suffered from serious problems for example,  vanishing gradients and exploding gradients, which prevented these RNN models from learning long ranged dependencies. This stimulated the building of more refined architectures intended to side step these obstacles.

% Literature Review
\subsection{Literature Review}
% Backpropagation Through Time
\subsubsection{Backpropagation Through Time}
BPTT is one of the most important algorithms used for training RNNs. Dating back to the original effort to expand the typical backpropagation algorithm, BPTT has been formulated to handle the difficulties of temporal sequences that are inherent in sequential data \parencite{werbos1990bptt}. This algorithm allows RNNs in learning sequence dependent data by unfold the network over time steps and then updating weights matrix through the gradient of loss function with respect to the variable \parencite{rumelhart1986backpropagation}.
\\[2ex]
\textbf{Conceptual Framework of BPTT}
\\[1ex]
BPTT works based on the technique of treating an RNN as a deep feedforward network for across multiple time steps. In the forward pass, the RNN, like other artificial neuronal network, applies operation over the data input in sequence, bringing changes in its own state variables at every time step, depending on the input and the previous state of its general working state or hidden state. This sequential processing produces outputs and stores the internal states of the network in any period \parencite{werbos1990bptt}.

This unfolds the RNN to construct a traditional Feedforward Neural Network where we can apply backpropagation through time. Below is the conceptual idea of BPTT in RNN.
\newpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{../Pic/pic1.png} % Replace "example-image" with your image file
    \caption{Unfolded RNN}
\end{figure}

\begin{longtable}{|c|c|c|}
    \hline
    \textbf{Notation} & \textbf{Meaning} & \textbf{Dimension}\\
    \hline
    $U$          & Weight matrix for input to hidden state       & $input\ size\times hidden\ unites$\\
    $W$          & Weight matrix for hidden to hidden state      & $hidden\ units\times hidden\ unites$\\
    $V$          & Weight matrix for hidden state to output state& $hidden\ units\times number\ of\ class$\\
    $x_t$        & Input vector at time t                        & $input\ size\times 1$\\
    $h_t$        & Hidden state output at time t                 & $hidden\ units\times 1$\\
    $b_h$        & Bias term for hidden state                    & $hidden\ units\times 1$\\
    $b_y$        & Bias term for output state                    & $number\ of\ class\times 1$\\
    $\hat{o}_y$  & Output at time t                              & $number\ of\ class\times 1$\\
    $\hat{y}_t$  & Output at time t                              & $hidden\ units\times 1$\\
    $\mathcal{L}$& Loss at time t                                & $scalar$\\
    \hline
    \caption{Unfolded RNN}
\end{longtable}
% Forward pass
\noindent \textbf{Forward Pass}
\\[1ex]
During the forward pass, the RNN processes the input sequence sequentially, computing hidden states and output at each timestep:
\begin{equation}
    h_t = f(U^Tx_{t}+W^Th_{t-1}+b_h)
\end{equation}
\begin{equation}
    \hat{y}_t = f(V^Th_t+b_y)
\end{equation}
\newline  % Computing the loss function
\noindent \textbf{Computing the loss function}
\\[1ex]
Assuming the loss is computed only at the final timestep t:
\begin{equation}
    \mathcal{L}_t = L(y_t, \hat{y}_t)
\end{equation}
In order to do backpropagation through time to tune the parameters in RNN, we need to calculate the partial derivative of loss function $\mathcal{L}$ with respect to the differently parameters.\\
\newline  % Backward pass using the chain rule
\noindent \textbf{Backward pass using the chain rule}
\\[1ex]
Using the chain rule for computing the gradient.\\
Partial derivative of loss function $\mathcal{L}$ with respect to $W$ (hidden to hidden state) at time 2. 
\begin{equation}
    \dfrac{\partial\mathcal{L}_2}{\partial W} = \dfrac{\partial\mathcal{L}_2}{\partial y_2} \cdot \dfrac{\partial y_2}{\partial h_2} \cdot \dfrac{\partial h_2}{\partial h_1} \cdot \dfrac{\partial h_1}{\partial W}
\end{equation}
By mathematic induction 
\begin{equation}
    \dfrac{\partial\mathcal{L}_t}{\partial W} = \dfrac{\partial\mathcal{L}_t}{\partial y_t} \cdot \dfrac{\partial y_t}{\partial h_t} \cdot (\sum_{i=1}^{t} \dfrac{\partial h_t}{\partial h_i} \cdot \dfrac{\partial h_i}{\partial W})
\end{equation}
Where 
\begin{equation}
    \dfrac{\partial h_t}{\partial h_i} = \prod_{j=i+1}^{t} \dfrac{\partial h_j}{\partial h_{j-1}}
\end{equation}

Partial derivative of loss function $\mathcal{L}$ with respect to $U$ (input to hidden state) at time 2.\\
\begin{equation}
    \dfrac{\partial\mathcal{L}_2}{\partial U} = \dfrac{\partial\mathcal{L}_2}{\partial y_2} \cdot \dfrac{\partial y_2}{\partial h_2} \cdot \dfrac{\partial h_2}{\partial h_1} \cdot \dfrac{\partial h_1}{\partial U}
\end{equation}
By mathematic induction 
\begin{equation}
    \dfrac{\partial\mathcal{L}_t}{\partial U} = \dfrac{\partial\mathcal{L}_t}{\partial y_t} \cdot \dfrac{\partial y_t}{\partial h_t} \cdot (\sum_{i=1}^{t} \dfrac{\partial h_t}{\partial h_i} \cdot \dfrac{\partial h_i}{\partial U})
\end{equation}
Where 
\begin{equation}
    \dfrac{\partial h_t}{\partial h_i} = \prod_{j=i+1}^{t} \dfrac{\partial h_j}{\partial h_{j-1}}
\end{equation}

Partial derivative of loss function $\mathcal{L}$ with respect to $V$ (hidden to output state) at time 2.\\
\begin{equation}
    \dfrac{\partial\mathcal{L}_2}{\partial V} = \dfrac{\partial\mathcal{L}_2}{\partial y_2} \cdot \dfrac{\partial y_2}{\partial h_2} \cdot \dfrac{\partial h_2}{\partial h_1} \cdot \dfrac{\partial h_1}{\partial V}
\end{equation}
By mathematic induction 
\begin{equation}
    \dfrac{\partial\mathcal{L}_t}{\partial V} = \dfrac{\partial\mathcal{L}_t}{\partial y_t} \cdot \dfrac{\partial y_t}{\partial h_t} \cdot (\sum_{i=1}^{t} \dfrac{\partial h_t}{\partial h_i} \cdot \dfrac{\partial h_i}{\partial V})
\end{equation}
Where 
\begin{equation}
    \dfrac{\partial h_t}{\partial h_i} = \prod_{j=i+1}^{t} \dfrac{\partial h_j}{\partial h_{j-1}}
\end{equation}

Partial derivative of loss function $\mathcal{L}$ with respect to $b_h$ (bias term in hidden state) at time 2.\\
\begin{equation}
    \dfrac{\partial\mathcal{L}_2}{\partial b_b} = \dfrac{\partial\mathcal{L}_2}{\partial y_2} \cdot \dfrac{\partial y_2}{\partial h_2} \cdot \dfrac{\partial h_2}{\partial h_1} \cdot \dfrac{\partial h_1}{\partial b_h}
\end{equation}
By mathematic induction 
\begin{equation}
    \dfrac{\partial\mathcal{L}_t}{\partial b_h} = \dfrac{\partial\mathcal{L}_t}{\partial y_t} \cdot \dfrac{\partial y_t}{\partial h_t} \cdot (\sum_{i=1}^{t} \dfrac{\partial h_t}{\partial h_i} \cdot \dfrac{\partial h_i}{\partial b_h})
\end{equation}
Where 
\begin{equation}
    \dfrac{\partial h_t}{\partial h_i} = \prod_{j=i+1}^{t} \dfrac{\partial h_j}{\partial h_{j-1}}
\end{equation}

Partial derivative of loss function $\mathcal{L}$ with respect to $b_y$ (bias term in output state) at time 2.
\begin{equation}
    \dfrac{\partial\mathcal{L}_2}{\partial b_y} = \dfrac{\partial\mathcal{L}_2}{\partial y_2} \cdot \dfrac{\partial y_2}{\partial h_2} \cdot \dfrac{\partial h_2}{\partial h_1} \cdot \dfrac{\partial h_1}{\partial b_y}
\end{equation}
By mathematic induction 
\begin{equation}
    \dfrac{\partial\mathcal{L}_t}{\partial b_y} = \dfrac{\partial\mathcal{L}_t}{\partial y_t} \cdot \dfrac{\partial y_t}{\partial h_t} \cdot (\sum_{i=1}^{t} \dfrac{\partial h_t}{\partial h_i} \cdot \dfrac{\partial h_i}{\partial b_y})
\end{equation}
Where 
\begin{equation}
    \dfrac{\partial h_t}{\partial h_i} = \prod_{j=i+1}^{t} \dfrac{\partial h_j}{\partial h_{j-1}}
\end{equation}

\textbf{parameters updates}
\begin{equation}
    W \leftarrow W - \alpha\dfrac{\partial\mathcal{L}}{\partial W}
\end{equation}
\begin{equation}
    U \leftarrow U - \alpha\dfrac{\partial\mathcal{L}}{\partial U}
\end{equation}
\begin{equation}
    V \leftarrow V - \alpha\dfrac{\partial\mathcal{L}}{\partial V}
\end{equation}
\begin{equation}
    b_h \leftarrow b_h - \alpha\dfrac{\partial\mathcal{L}}{\partial b_h}
\end{equation}
\begin{equation}
    b_y \leftarrow b_y - \alpha\dfrac{\partial\mathcal{L}}{\partial b_y}
\end{equation}

\newpage
\textbf{Pseudocode of BPTT} \parencite{wikipedia2023bptt}
\begin{algorithm}[H]
    \caption{Backpropagation Through Time (BPTT)}
    \begin{algorithmic}[1]
    \STATE \textbf{Input:} 
    \STATE \hspace{1em} Sequence of input data $\{x_1, x_2, \dots, x_T\}$
    \STATE \hspace{1em} Sequence of target outputs $\{y_1, y_2, \dots, y_T\}$
    \STATE \hspace{1em} Learning rate $\eta$
    \STATE \hspace{1em} Number of time steps to unroll $N$
    \STATE \textbf{Initialize:} Model parameters $\theta$, hidden state $h_0 = 0$
    
    \STATE \textbf{Forward Pass:}
    \FOR{$t = 1$ to $T$}
        \STATE Compute hidden state: $h_t = f(h_{t-1}, x_t; \theta)$
        \STATE Compute output: $\hat{y}_t = g(h_t; \theta)$
        \STATE Compute loss for time step $t$: $L_t = \mathcal{L}(\hat{y}_t, y_t)$
    \ENDFOR
    
    \STATE \textbf{Backward Pass (BPTT):}
    \STATE Set total loss: $L = \sum_{t=1}^{T} L_t$
    \FOR{$t = T$ down to $1$}
        \STATE Compute gradient of loss with respect to output: $\frac{\partial L_t}{\partial \hat{y}_t}$
        \STATE Backpropagate through output layer to obtain: $\frac{\partial L_t}{\partial h_t}$
        \STATE Accumulate gradients for parameters: $\frac{\partial L}{\partial \theta}$
        \FOR{$k = 1$ to $N$}
            \STATE Backpropagate through time for $N$ steps:
            \STATE Compute gradient contribution from step $t-k$: $\frac{\partial L_t}{\partial h_{t-k}}$
        \ENDFOR
    \ENDFOR
    
    \STATE \textbf{Update Parameters:}
    \STATE $\theta = \theta - \eta \cdot \frac{\partial L}{\partial \theta}$
    
    \STATE \textbf{Output:} Updated parameters $\theta$
    \end{algorithmic}
\end{algorithm}

% Actication function
\subsubsection{Activation Function}
Activation functions, particularly the sigmoid function, are fundamental components of recurrent neural networks (RNNs). They transform input data into output data. A key property of these functions is their differentiability. Differentiability is crucial for the backpropagation through time (BPTT) algorithm, enabling the application of the chain rule during training. \\[1ex]
% Sigmoid Activation Function 
\textbf{Sigmoid activation function}
\\[1ex]
The main role of the sigmoid activation function is to normalize candidate values and convert the cell state to a hidden state when performing cell state updates. It limits the output between [0,1] because it has a smooth gradient, which is important for discovering long-range dependencies.\\[1ex]
\begin{equation}
    Sigmoid(x) = \frac{ 1 }{ 1+e^{-x} }
\end{equation}
\begin{equation}
    Sigmoid'(x) = Sigmoid(x)(1-Sigmoid(x))
\end{equation}
Below is the sigmoid function and its derivative.
\newpage
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\textwidth]{../Pic/sigmoid.png} % Replace "example-image" with your image file
\end{figure}

$Domain(Sigmoid(x))=\mathbb{R},\hspace{1em} Codomain(Sigmoid(x))=(0,1)$\\
$Domain(Sigmoid'(x))=\mathbb{R},\hspace{1em} Codomain(Sigmoid'(x))=[0,0.5]$
\newline

% Hyperbolic tangent activation function
\textbf{Hyperbolic tangent activation function}
\\[1ex]
The main role of the hyperbolic tangent (tanh) activation function is to normalize candidate values and convert the cell state to a hidden state when performing cell state updates. It limits the output between [-1,1] because it has a stable gradient, which is important for discovering long-range dependencies.
\newline
\begin{equation}
    tanh(x) = \frac{ e^{x} - e^{-x} }{e^{x} + e^{-x}}
\end{equation}
\begin{equation}
    tanh'(x) = 1 - tanh^{2}(x)
\end{equation}
Below is the Hyperbolic tangent activation function and its derivative.
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.8\textwidth]{../Pic/tanh.png} % Replace "example-image" with your image file
\end{figure}

$Domain(tanh(x))=\mathbb{R},\hspace{1em} Codomain(tanh(x))=[-1,1]$\\
$Domain(tanh'(x))=\mathbb{R},\hspace{1em} Codomain(tanh'(x))=[0,1]$

% Gradient vanishing and gradient exploring
\subsubsection{Gradient vanishing and gradient exploring}
When training the RNN, BPTT was used to update the weight matrix. As the number of time steps increase, the problem of gradient instability of often encountered, and this problem is gradient vanishing and gradient exploring \parencite{bengio1994learning}. 
\\[2ex]
% Vanishing Gradients
\textbf{Vanishing Gradients}
\\[1ex]
Generally, sigmoid activation function is used commonly in RNNs, has a maximum derivative of 0.25. When doing BPTT in long time steps, this multiplication results in exponentially diminishing gradients as the sequence length increases. Consequently, the shallow neural receive very small gradient updates, making it difficult to adjust the parameters effectively. This leads to the model struggling to learn long time dependencies. 
\\[1ex]
% Exploding Gradients
\textbf{Exploding Gradients}
\\[1ex]
When we are doing the feedforward and get super large value computed by loss function. Then when updating the parameters. The updates to the weights will also be large. Resulting in higher loss and larger gradients in the next iterations. This will lead to exploding gradients.
\\[1ex]
We have introduce the backpropagation through time. This is the method to update the parameters in RNNs. When calculating, for example, the partial derivative of loss function with respect to W. Assume the time $t$ go to infinity large. We will get this term. $ \prod_{j=i+1}^{t} \dfrac{\partial h_j}{\partial h_{j-1}} $, and it will lead to exponential problem. if $ \dfrac{\partial h_j}{\partial h_{j-1}} > 1 $. Then the product of all term will increase exponentially, then exploding gradients occur. On the contrary, if $\dfrac{\partial h_j}{\partial h_{j-1}} < 1$. Then the result will decrease exponentially, then vanishing gradients occur.
\begin{figure}[!h]
    \centering
    \includegraphics[width=1\textwidth]{../Pic/exponential_growth.png} % Replace "example-image" with your image file
\end{figure}


% Long short-term memory (LSTM)
\subsubsection{Long short-term memory (LSTM)}
Long short-term memory proposed by \parencite{hochreiter1997lstm}. LSTM is designed for handling long time step problems. The architecture of LSTM can prevent vanishing gradient and exploring gradient. The main difference between LSTM and RNN is the number of gates. LSTM introduced input, forget and output gates. This allows LSTM to manage the flow of information more effectively, retaining important information over longer sequences.
\\[2ex]
Architecture
\\[1ex]
LSTMs introduce a memory cell that can maintain information over long time steps the cell is controlled by three gates, input gate, output gate, and forget gate. Each cell of LSTMs inside has 3 sigmoid and 1 tanh layer. Below graph unfolds the LSTM hence we can analyze different gates.
\begin{figure}[!htb]
    \centering
    \includegraphics[width=1\textwidth]{../Pic/lstm1.png} % Replace "example-image" with your image file
\end{figure}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=1\textwidth]{../Pic/lstm2.png} % Replace "example-image" with your image file
\end{figure}
\\
% Forget gate
Forget gate:
\\[1ex]
The forget gate is a component of the LSTM, designed to manage the flow of information within the cell state. The function of forget gate is to determine which information should be retained in memory cell \parencite{hochreiter1997lstm}.
\begin{equation}
    f_t = \sigma( W_f \cdot [ h_{t-1} , x_t ] + b_f ) 
\end{equation}
% Input gate
Input gate: 
\\[1ex]
The input gate controls how much new information from the current time step is allowed to enter the cell \parencite{hochreiter1997lstm}. For the $\widetilde{C}_t$, the purpose is to suggest updates for the cell state. 
\begin{equation}
    i_t = \sigma( W_i \cdot [ h_{t-1} , x_t ] + b_c ) 
\end{equation}
\begin{equation}
    \widetilde{C}_t = tanh( W_c \cdot [ h_{t-1} , x_t ] + b_c )
\end{equation}
% Cell state update
Cell State Update:
\\[1ex]
The forget gate will drop the meaningless information and add some potential information.
\begin{equation}
    C_t = f_t * C_{t-1} + i_t * \widetilde{C_t}
\end{equation}
% Output gate
Output gate:
\\[1ex]
The output gate is able to control how much or what information from the cell state should be passed to the next layer or used in predictions. 
\begin{equation}
    o_t = \sigma( W_o \cdot [ h_{t-1} , x_t ] + b_o )
\end{equation}
% Hidden state update
Hidden State Update:
\\[1ex]
The hidden state is influenced by output value and current cell state. 
\begin{equation}
    h_t = o_t * tanh(C_t)
\end{equation}
\\[1ex]
n = number of features in the input vector $x_t$.\\
m = number of units in LSTM.\\
\begin{longtable}{|c|c|c|}
    \hline
    \textbf{Notation} & \textbf{Meaning} & \textbf{Dimension}\\
    \hline
    $x_t$               & Input vector at time t                        & $n \times 1$\\
    $h_t$               & Hidden state output at time t                 & $m \times 1$\\
    $C_t$               & Cell state at time t                          & $m \times 1$\\
    $f_t$               & Forget gate output at time t                  & $m \times 1$\\
    $i_t$               & Input gate output at time t                   & $m \times 1$\\
    $o_t$               & Output gate output at time t                  & $m \times 1$\\
    $\widetilde{C_t}$   & Candidate memory cell at time t               & $m \times 1$\\
    $W_f$               & Weight matrix for the forget gate             & $m \times (m+n)$\\
    $W_i$               & Weight matrix for the input gate              & $m \times (m+n)$\\
    $W_C$               & Weight matrix for the candidate memory cell   & $m \times (m+n)$\\
    $W_o$               & Weight matrix for the output gate             & $m \times (m+n)$\\
    $b_f$               & Bias vector for the forget gate               & $m \times 1$\\
    $b_i$               & Bias vector for the input gate                & $m \times 1$\\
    $b_C$               & Bias vector for the candidate memory cell     & $m \times 1$\\
    $b_o$               & Bias vector for the output gate               & $m \times 1$\\
    \hline
    \caption{Unfolded RNN}
\end{longtable}

Number of parameters:\\
\begin{enumerate}
    % 1. Weights matrix for the input
    \item Weights matrix for the input
    \begin{itemize}
        \item Forget gate: $n \times m$
        \item Input gate: $n \times m$
        \item Cell gate: $n \times m$
        \item Output gate: $n \times m$
    \end{itemize}

    % 2. Weight matrix for the hidden state
    \item Weight matrix for the hidden state
    \begin{itemize}
        \item Hidden state for forget gate: $m \times m$
        \item Hidden state for input gate: $m \times m$
        \item Hidden state for cell gate: $m \times m$
        \item Hidden state for output gate: $m \times m$
    \end{itemize}

    % 3. Bias term
    \item Bias term
    \begin{itemize}
        \item Bias for forget gate: $1 \times m$
        \item Bias for input gate: $1 \times m$
        \item Bias for cell gate: $1 \times m$
        \item Bias for output gate: $1 \times m$
    \end{itemize}
\end{enumerate}
Total parameters: $4 \times ( n + m + 1 ) \times m$


% Gated Recurrent Unit (GRU)
\subsubsection{Gated Recurrent Unit (GRU)}
The gated recurrent unit (GRU) was proposed by \parencite{cho2014properties} to make each recurrent unit to adaptively capture dependencies of different time scales. The GRU has 2 gates, update gate and reset gate. 
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.65\textwidth]{../Pic/gru.png} % Replace "example-image" with your image file
\end{figure}
Update gate:
\\[1ex]
The update gate determines how much of the past information should be retained in the current hidden state. 
\begin{equation}
    z_t = \sigma(W_z \cdot [ h_{t-1} , x_t ] + b_z )
\end{equation}
% Reset gate
Reset gate:
\\[1ex]
The reset gate is similar with the update gate, but the candidate hidden state is influenced by the reset gate. 
\begin{equation}
    r_t = \sigma( W_r \cdot [ h_{t-1} , x_t ] + b_r )
\end{equation}
Candidate hidden state:
\\[1ex]
The candidate hidden state combined with previous hidden state and current input to form the potential new information that can be added to the current hidden state.
\begin{equation}
    \widetilde{h_t} = tanh ( W_h \cdot [ r_t * h_{t-1} , x_t ] + b_h )
\end{equation}
Final hidden state
\\[1ex]
The final hidden state of the GRU at time t is a linear interpolation between the previous final hidden state and the candidate hidden state. 
\begin{equation}
    h_t = ( 1 - z_t ) * h_{t-1} + z_t * \widetilde{ h_t }
\end{equation}

\begin{longtable}{|c|c|c|}
    \hline
    \textbf{Notation} & \textbf{Meaning} & \textbf{Dimension}\\
    \hline
    $x_t$               & Input vector at time t                        & $n \times 1$\\
    $h_t$               & Hidden state output at time t                 & $m \times 1$\\
    $r_t$               & Reset gate output at time t                   & $m \times 1$\\
    $z_t$               & Update gate output at time t                  & $m \times 1$\\
    $W_z$               & Weight matrix for the update gate             & $m \times (m+n)$\\
    $W_r$               & Weight matrix for the candidate memory cell   & $m \times (m+n)$\\
    $W_h$               & Weight matrix for the output gate             & $m \times (m+n)$\\
    $b_z$               & Bias vector for the input gate                & $m \times 1$\\
    $b_r$               & Bias vector for the candidate memory cell     & $m \times 1$\\
    $b_h$               & Bias vector for the output gate               & $m \times 1$\\
    \hline
\end{longtable}

Number of parameters:\\
\begin{enumerate}
    % 1. Weights matrix for the input
    \item Weights matrix for the input
    \begin{itemize}
        \item Update gate: $n \times m$
        \item Reset gate: $n \times m$
        \item Candidate hidden state: $n \times m$
    \end{itemize}

    % 2. Weight matrix for the hidden state
    \item Weight matrix for the hidden state
    \begin{itemize}
        \item Hidden state for update gate: $m \times m$
        \item Hidden state for reset gate: $m \times m$
        \item Hidden state candidate hidden state: $m \times m$
    \end{itemize}

    % 3. Bias term
    \item Bias term
    \begin{itemize}
        \item Bias for update: $1 \times m$
        \item Bias for reset: $1 \times m$
        \item Bias for candidate hidden state: $1 \times m$
    \end{itemize}
\end{enumerate}
Total parameters: $3 \times ( n + m + 1 ) \times m$


% Deep recurrent neural networks (DRNNs)
\subsubsection{Deep recurrent neural networks (DRNNs)}
Deep architecture networks with multiple layers that can hierarchically learn complex representations \parencite{bengio2009learning}. By extending of this concept, stacking recurrent layers to form Deep Recurrent Neural Networks aligns with this principle. A number of research papers have been prove that the performance of DRNNs is out-performance then conventional RNNs. \parencite{le2010deep, delalleau2011shallow, pascanu2013on}
\\[1ex]
The architecture of DRNNs are similar with conventional RNNs. We simply stack the recurrent layers vertically. For the first layer, this layer receives the input and combines it with its previous hidden state $h_{t-1}$ (Equation 38). The second layer receive the hidden state of the first layer and treat as the input of the layer 2 (Equation 39). We can extend this concept to $L$ layers (Equation 40).
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.5\textwidth]{../Pic/drnn.png} % Replace "example-image" with your image file
\end{figure}
\\[1ex]
\begin{equation}
    h^{(1)}_t = f(W^{(1)}_{xh} x_{t} + W^{(1)}_{hh} h^{(1)}_{t-1} + b^{(1)}_h)
\end{equation}
\begin{equation}
    h^{(2)}_t = f(W^{(2)}_{xh} h^{(1)}_{t} + W^{(2)}_{hh} h^{(2)}_{t-1} + b^{(2)}_h)
\end{equation}
\begin{equation}
    h^{(L)}_t = f(W^{(L)}_{xh} h^{(L-1)}_{t} + W^{(L)}_{hh} h^{(L)}_{t-1} + b^{(L)}_h)
\end{equation}
\begin{equation}
    o_t = W_{hy}h^{(L)}_{t} + b_y
\end{equation}
\begin{equation}
    y_t = g(o_t)
\end{equation}
Where $x_t$ is the input at time $t$. $h^{(l)}_{t}$ is the hidden state for the $l$ layer at time t. $W^{(l)}_{xh}, W^{(l)}_{hh}$ are the weight matrices for the input to hidden and hidden to hidden connections in layer $l$, respectively. $W_{hy}$ is weight matrix for the output layer. $b^{(l)}_h$ is the bias vector for the $l$ layer (except output layer), $b_y$ is the bias vector for output layer. $g(\cdot)$ and $f(\cdot)$ are an activation function. 

\subsubsection{Hidden Markov Model}
Before reviewing Hidden Markov Model (HMM), it is essential to understand what Markov models is, or Markov chains. Markov chains are fundamental models in probability theory and statistic, and it is a stochastic process describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. This property known as the Markov property \parencite{roberts2004general, rabiner1986anintroduction}. In rigorous terms. Let the state space be defined as:
\begin{equation}
    S = \{ s_1, s_2, \dots, s_N \}.
\end{equation}
\begin{equation}
    P(s_{t+1} \mid s_t, s_{t-1}, s_{t-2}, \cdots, s_{1}) = P(s_{t+1} \mid s_t)
\end{equation}
The state transition probability is denoted by:
\begin{equation}
    P_{ij} = P(s_{t+1} = s_j \mid s_t = s_i).
\end{equation}

Given an initial state \( s_1 \) with probability \( P(s_1) \), the joint probability of a state sequence \( \{s_1, s_2, \dots, s_T\} \) can be written as:
\begin{equation}
    P(s_1, s_2, \dots, s_T) = P(s_1) \cdot P(s_2 \mid s_1) \cdot P(s_3 \mid s_2) \cdots P(s_T \mid s_{T-1})
\end{equation}
We can simplfy the joint probability of a state sequence as equestion 47.
\begin{equation}
    P(s_1, s_2, \dots, s_T) = P(s_1) \prod_{t=1}^{T-1} P(s_{t+1} \mid s_{t})
\end{equation}
Then it is essential to discuss marginal probability in a Markov model because it provides critical information about the likelihood of being in particular state $s_j$ at a specific time $t$. The marginal probability of being in state $s_j$ at time $t$ defined as:
\begin{equation}
    \pi_t(s_j) = P(s_t=s_j)
\end{equation}
It is the probability of the system being in state $s_j$ at time t, irrespective of how the system transitioned to $s_j$. We can obtainde by summing over all possible transition probability $P_{ij}$ where $s_i$ belong in state space. 
\begin{equation}
    \pi_{t+1}(s_j) = \sum_{s_i \in S} \pi_t(s_i) \cdot P_{ij}
\end{equation}
If Markov chain is irreducible and aperiodic, the stationary distribution defined as below:
\begin{equation}
    \pi_j = \sum_{i=1}^{n} \pi_i P_{ij}
\end{equation}



% Hidden markov model 
\textbf{Hidden markov model}
\\[1ex]
Hidden Markov Model (HMMs) is a statistical model that extends the Markov chains by introducting a layer of latent (unobservable) variables. And connecting them to observable outputs (emission probability). In contrast to standard Markov chains, hidden states are never be observed directly. Instead, we observe a sequence of data (observations) that are probabilitically generated by these hidden state. This structure has proven useful for applications in speech recognition, biological sequence analysis, and signal processing \parencite{rabiner1989atutorial}. Lets the hidden states be denoted by $X_t$, where $X_t$ is the hidden state at time $t$. The hidden state space is defined as:
\begin{equation}
    \mathcal{X} = \{s_1, s_2, \cdots, s_n\}
\end{equation}
where $\mathcal{X}$ is finite set of size $n$, and each $s_i \in \mathcal{X}$ represents a specific possible hidden state. Then for the observable space represents the possible outcomes (observations) that are generated by the hidden state, we have discuss previously. Let the observations at time $t$ be denoted by $Y_t$. The observable space is defined as:
\begin{equation}
    \mathcal{Y} = \{o_1, o_2, \cdots, o_m\}
\end{equation}
where $\mathcal{Y}$ is finite set of size $m$, and each $o_i \in \mathcal{Y}$ represents a specific possible observation. 
\\[1ex]
Then for the transition probability matrix ($P$), the probability of transitioning between hidden states denoted as:
\begin{equation}
    P_{n \times n} = 
    \left[ {\begin{array}{cccc}
        p_{11} & p_{12} & \cdots & p_{1n}\\
        p_{21} & p_{22} & \cdots & p_{2n}\\
        \vdots & \vdots & \ddots & \vdots\\
        p_{n1} & p_{n2} & \cdots & p_{nn}\\
    \end{array} } \right]
\end{equation}
\begin{equation}
    p_{ij} = P(X_{t+1} = s_j \mid X_t = s_i) 
\end{equation}
where $s_j, s_i \in \mathcal{X}$.
\\[1ex]
For emission probability matrix ($B$), the probabilities of generating an observation given a hidden state.
\begin{equation}
    B_{n \times n} = 
    \left[ {\begin{array}{cccc}
        b_{11} & b_{12} & \cdots & b_{1m}\\
        b_{21} & b_{22} & \cdots & b_{2m}\\
        \vdots & \vdots & \ddots & \vdots\\
        b_{m1} & b_{m2} & \cdots & b_{mm}\\
    \end{array} } \right]
\end{equation}
\begin{equation}
    b_{ik} = P(Y_{t} = o_k \mid X_t = s_i) 
\end{equation}
where $s_i \in \mathcal{X}, o_k \in \mathcal{Y}$.
\\[1ex]
And the goal of hidden markov model is the maximize the joint probability $P(X, Y)$ given the initial probability $P(X_1)$.
\begin{equation}
    P(X, Y) = P(X_1) \cdot \prod_{t=1}^{T-1}P(X_{t+1}=s_j \mid X_{t}=s_i) \cdot \prod_{t=1}^{T} P(Y_t=o_k \mid X_t=s_i)
\end{equation}

For the NLP tasks, there may have some unobservable data, for example the implicit sentiment, user intent, emotional state. \parencite{rabiner1993fundamentals} proposes the hidden Markov model (HMM), the assumption is the existence of the latent process follows a Markov chain from which observations X are generated. In other word, there would exists an unobserved state sequence $Z = \{z_1, z_2 \cdots , z_T\}$ in observed sequence $X = \{x_1, x_2, \cdots, x_T\}$ \parencite{sengupta2023hybrid}. Where the hidden states, $z_t$ belonging to state-space $Q = \{q_1, q_2, \cdots, q_M \}$ follow a Markov chain goverened by: 
\begin{itemize}
    \item A state-transition probability matrix $A = [a_{ij}] \in \mathbb{R}^{M \times M}$ where $a_{ij} = p(z_{t+1} = q_j | z_t = q_i)$ 
\end{itemize}
\begin{itemize}
    \item Initial state matrix $\pi = [\pi_i] \in \mathbb{R}_{1 \times M}$ with $\pi_i = p(z_1 = q_i)$
\end{itemize}
Furthermore, for the hidden state $z_t$, correspoinding to the observe data $x_t$ is release by emission process $B = [b_j(x)]$ where $b_j(x) = p(x|z=q_i)$. We can assume $b_j(x)$ is follows the Gaussian mixture model (GMM).
\begin{equation}
    p(x_t|z=q_j) = \sum_{l=1}^{k}c_{jl}\mathcal{N}(x_t|\mu_{jl}, \Sigma_{jl})
\end{equation}
where $\sum_{l=1}^{k}c_{jl} = 1$, $\forall j = \{1,\cdots, M\}$, k is the number of Gaussian mixture components and $\mathcal{N}(x_t|\mu_{jl}, \Sigma_{jl})$ denotes a Gaussian probability density with mean $\mu_{jl}$ and covariance $\Sigma_{jl}$ for state $j$ and mixture component $l$. The number of hidden states ($M$) and mixture component ($k$) are the two hyperparameters of the model which have to be provided apriori.
\\[1ex]
Therefore, the joint probability probability density function of the observation $X$ can be expressed as:
\begin{equation}
    p(X) = p(z_1) \prod_{t=1}^{T-1} p(z_{t+1}|z_{t}) \prod_{t=1}^{T} p(x_t|z_t)
\end{equation}
The optimal parameters $[A, B, \pi]$, which maximize the likelihood of the observation sequence $X$ (Equation 39), are determined using the Baum-Welch algorithm, an expectation-maximization method \parencite{rabiner1993fundamentals}. Additionally, the probability of the system in a specific hidden state $z_t$ corresponding to the observation $x_t$ is calculated using the Viterbi algorithm.



% Word representation 
\subsubsection{Word representation}
In RNN, the due to the architecture, the model can only handle vector, it is important that convert the word into machine readable format. By translating words into vectors, these representations capture semantic meanings, relationships and contexts. \parencite{mikolov2013efficient} proposed 2 model architectures for leaning distributed representations of words.
\\[1ex]
\textbf{Continuous Bag of Words model}
\\[1ex]
CBOW operates by taking context words around a target word, it is flexible to adjust the window size which mean that we can control how many words around our target word. Hence aggregating their embeddings and passing to hidden layer to produce a probability distribution. Capture the semantic meanings and the relationship more effectively. 
\begin{equation}
    Q = N \times D + D \times log_2(V)
\end{equation}
\\[1ex]
\textbf{Continuous Skip gram model}
\\[1ex]
The continuous skip gram model operates in opposite direction compare with CBOW. The model takes the surrounding word to predict the target word. In other word, treat the current word as an input to a log-linear classifier with continuous projection layer, and predict the probability distribution of surrounding words.
\begin{equation}
    Q = C \times (D + D \times log_2(V))
\end{equation}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=1\textwidth]{../Pic/word_representation.png} % Replace "example-image" with your image file
\end{figure}


% ######################## Background and Literature Review #########################

% ######################## Project Goals and Objectives ######################################
\section{Project Goals and Objectives}

% Project Goals
\subsection{Project Goals}
The primary goal of this report is to deeply review on RNN and how it can apply in natural language processing (NLP) tasks. And by comparing the performance of different models. this report aims to provide a detailed comparative analysis of the following RNN-based models:
\begin{itemize}
    \item Vanilla RNN
    \item Long Short-Term Memory (LSTM)
    \item Gated Recurrent Unit (GRU)
\end{itemize}

% Objectives
\subsection{Objectives}
\subsubsection{Implement Existing RNN Architectures}
Develop models using state-of-the-art deep learning framework, PyTorch, to implement each of the above RNNs model. This includes setting up the network layers, loss functions, and appropriate optimization methods.
\subsubsection{Apply the Models on a Benchmark NLP Dataset}
Utilize a selected NLP dataset to train and evaluate the performance of each RNN-based model. The dataset will be preprocessed (e.g., tokenization, padding, embedding initialization) to suit sequential data modeling.
\subsubsection{Compare Model Performances}
Compare different RNN-based models on metrics relevant to the target NLP taskquantitatively and qualitatively. Accuracy, F1, training time scores be used.
\subsubsection{Analyze the Impact of Architectural Differences}
Identify and discuss the strengths and limitations of each model architecture through both experimental results and theoretical insights. For example, compare how LSTM and GRU architectures mitigate vanishing gradients and exploding gradients using gating mechanisms relative to a vanilla RNN.


% ######################## Project Goals and Objectives ######################################

% ######################## Research Plan / Methodology ######################################
\section{Research Plan / Methodology}
To achieve the project goals and objectives, this report will follow the step as below:
\subsection{Literature Review}
Since this report require some knowledge related on RNN and NLP. So it is necessary to review the foundemental of those area. Different architecture of RNNs-based model for instance, vanilla RNN, LSTM, GRU. And understand the limitation of RNN, vanishing/exploding gradients. Also, it is essential to understand natural language processing (NLP).  
\subsection{Data Collection and Preprocessing}
\subsubsection{Data Collection}
In this report, ACL-IMDB \parencite{maas-EtAl:2011:ACL-HLT2011} was experiment. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. This dataset provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. I preporcess the dataset by text cleaning and tokenization.
\subsubsection{Data Preprocessing}
\begin{enumerate}
    \item Lowercasing:\\
    All text was converted to lowercase to ensure consistency and avoid treadting words like "Good" and "good" as different tokens.
    \item Handing Abbreviations:\\
    I hard code a dictionary in python, and marjority of common abbreviations were included and replaced with their full forms. For example:
    \begin{itemize}
        \item "u" -$>$ "you"
        \item "pls" -$>$ "please"
    \end{itemize}
    \item Removing HTML Tags:\\
    HTML tags were removed using regular expression 
    \begin{lstlisting}[language=Python] 
        import re
        re.sub(r'<.*?>', '', sentence) 
    \end{lstlisting}
    where variable "sentence" is movie review. 
    \item Removing Special Characters\\
    Non-alphanumeric characters (e.g. punctuation) were removed using the regular expression
    \begin{lstlisting}[language=Python]
        import re
        re.sub(r'[^a-zA-Z0-9\s]', '', sentence)
    \end{lstlisting}
    \item Removing Extra Whitespace\\
    Leading and trailing whitespaces were stripped. 
    \item Removing Stop Words\\
    Stop words (e.g., "is", "the", "and") were removed to reduce noise in the text. The NLTK library's stop word list was used. Words not in the stop word list were retained.
    \item Tokenization\\
    The cleaned text was split into individual tokens, I use white space and the criterion to split the word.
\\[1ex]
Once the text was preprocessed, a vocabulary was constructed from the dataset. The vocabulary mapping each word to a unique index. Words not found in the vocabulary are replaced with an "unknown" token. I reserved index 0 as unknown token. So that if the word can not map to correspoinding index. Then the token should be 0. This is a good way to handle Out-of-vocabulary words \parencite{mikolov2013efficient}.    
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Word} & \textbf{Index} \\ \hline
        \texttt{<UNK>} & 0 \\ \hline
        acting & 1 \\ \hline
        good & 2 \\ \hline
        movie & 3 \\ \hline
        amazing & 4 \\ \hline
        performance & 5 \\ \hline
        bad & 6 \\ \hline
        terrible & 7 \\ \hline
        excellent & 8 \\ \hline
    \end{tabular}
    \caption{Word to Index Mapping Table}
    \label{tab:word_index}
\end{table}
\end{enumerate}
\subsection{Model Architecture}
In this section, i will describe the model architecture, i will list out all of the model i have used and the detail information of those models. For mathematical notation, i will define all of the variables precisely and clearly. And because this project mainly focusing on natural language processing task. So that the classification model consists of an embedding layer, this is just like a look-up table to convert token to vector. First, the sequence of words $(w_1, w_2, \cdots, w_T)$ are passed through an embedding layer, which is the look-up table to convert those words in vector format. In other words, the look-up table map the word in a high-demensional space $(v_1, v_2, \cdots, v_T)$ (Table 5), where $v_t \in \mathbb{R}^d$. And the dimension of the vector space is depends on how we define the hidden size of the model. Next, the forward of RNN based model will processes these word vectors $v_t$ in the forward directions, updating corresponding hidden states at each time step $t$. 

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Word} & \textbf{Index} & \textbf{Embedding Vector} \\ \hline
        \texttt{<UNK>} & 0 & $\begin{bmatrix} 0.01 & -0.02 \cdots 0.03 & 0.04 \end{bmatrix}$ \\ \hline
        acting & 1 & $\begin{bmatrix} 0.12 & 0.23 \cdots -0.11 & 0.05 \end{bmatrix}$ \\ \hline
        good & 2 & $\begin{bmatrix} 0.03 & -0.15 \cdots 0.08 & 0.07 \end{bmatrix}$ \\ \hline
        movie & 3 & $\begin{bmatrix} 0.10 & 0.05 \cdots -0.09 & -0.01 \end{bmatrix}$ \\ \hline
        amazing & 4 & $\begin{bmatrix} 0.25 & -0.12 \cdots 0.18 & 0.30 \end{bmatrix}$ \\ \hline
        performance & 5 & $\begin{bmatrix} -0.10 & -0.20 \cdots 0.15 & -0.05 \end{bmatrix}$ \\ \hline
        bad & 6 & $\begin{bmatrix} 0.40 & 0.35 \cdots -0.10 & 0.50 \end{bmatrix}$ \\ \hline
        terrible & 7 & $\begin{bmatrix} 0.30 & 0.25 \cdots -0.11 & 0.40 \end{bmatrix}$ \\ \hline
        excellent & 8 & $\begin{bmatrix} 0.32 & 0.38 \cdots -0.90 & 0.70 \end{bmatrix}$ \\ \hline
    \end{tabular}
    \caption{Word to Index Matching with Lookup Table Embeddings}
    \label{tab:word_lookup}
\end{table}

\subsubsection{Model 1 (Vanilla RNN)}
This is the vanilla RNN model, with 1 layer. We just use the last output to calculate the loss function and do the backpropagation.
\begin{center} % Center the graph on the page
    \begin{tikzpicture}[
        node distance=1.5cm and 2cm, % Distance between nodes
        every node/.style={draw, minimum size=1cm, font=\small, align=center}, % Node style
        arrow/.style={-Latex, thick} % Arrow style
    ]

    % Input nodes
    \node (v1) [circle, fill=blue!10] {$v_1$};
    \node (v2) [circle, fill=blue!10, right=of v1] {$v_2$};
    \node (v3) [circle, fill=blue!10, right=of v2] {$v_3$};

    % Hidden nodes
    \node (h1) [circle, fill=green!10, above=of v1] {$h_1$};
    \node (h2) [circle, fill=green!10, above=of v2] {$h_2$};
    \node (h3) [circle, fill=green!10, above=of v3] {$h_3$};

    % Output nodes
    \node (y1) [circle, fill=red!10, above=of h1] {$y_1$};
    \node (y2) [circle, fill=red!10, above=of h2] {$y_2$};
    \node (y3) [circle, fill=red!10, above=of h3] {$y_3$};

    % Horizontal connections (input to hidden)
    \draw[arrow] (v1) -- (h1);
    \draw[arrow] (v2) -- (h2);
    \draw[arrow] (v3) -- (h3);

    % Horizontal connections (hidden to output)
    \draw[arrow] (h1) -- (y1);
    \draw[arrow] (h2) -- (y2);
    \draw[arrow] (h3) -- (y3);

    % Recurrent connections
    \draw[arrow] (h1.east) to[out=30, in=150] (h2.west);
    \draw[arrow] (h2.east) to[out=30, in=150] (h3.west);

    % Labels
    \node[above=0.3cm of y2, font=\bfseries] {Recurrent Neural Network (Unfolded)};
    \node[below=0.5cm of v2, font=\itshape] {Time Steps};

    \end{tikzpicture}
\end{center}

\begin{equation}
    h_t = f(W_{ih}^Tv_t+W_{hh}h_{t-1}+b_h)
\end{equation}
where $W_{ih}$ is input to hidden matrix, $v_t$ is the word vector at time step $t$, $W_{hh}$ is hidden to hidden matrix. $h_t$ is hidden state at time $t$. $b_h$ is biased term for hidden state. For the output at time $t$, we will pass the hidden state output to an activation function defined as below. 
\begin{equation}
    \hat{y}_t = f(W_{ho}^Th_{t}+b_y)
\end{equation}
where $y_t$ is the output at time step $t$ and $W_{ho}$ is the hidden to output matrix and $b_y$ is biased term for output. And $f(\cdot)$ is an activation function. Then we calculate the loss function by the last output.
\begin{equation}
    Loss = \mathcal{L}(\hat{y}, y)
\end{equation}

\subsubsection{Model 2 (Long short-term memory)}
For the LSTM model, I use 1 layer:
\begin{figure}[H]
    \centering
    \fbox{
    \resizebox{0.95\textwidth}{!}{
    \begin{tikzpicture}[
    font=\large,
        >=LaTeX,
        cell/.style={% For the main box
        rectangle, 
        rounded corners=5mm, 
        draw,
        very thick,
        },
        operator/.style={
        circle,
        draw,
        inner sep=-0.5pt,
        minimum width =.3cm,
        minimum height =.3cm,
        },
        function/.style={
        ellipse,
        draw,
        inner sep=1pt
        },
        ct/.style={
        circle,
        draw,
        line width = .75pt,
        minimum width=1cm,
        inner sep=1pt,
        },
        gt1/.style={
        rectangle,
        draw,
        fill=green, 
        minimum width=4mm,
        minimum height=3mm,
        inner sep=1pt
        },
        gt2/.style={
        rectangle,
        draw,
        fill=red, 
        minimum width=4mm,
        minimum height=3mm,
        inner sep=1pt
        },
        gt3/.style={
        rectangle,
        draw,
        fill=yellow, 
        minimum width=4mm,
        minimum height=3mm,
        inner sep=1pt
        },
        ArrowC1/.style={% Arrows with rounded corners
        rounded corners=.25cm,
        thick,
        },
        ArrowC2/.style={% Arrows with big rounded corners
        rounded corners=.5cm,
        thick,
        },
        cella/.pic={
            \node [cell, minimum height =4cm, minimum width=6cm] at (0,0){};
            \node [gt1] (ibox1a) at (-2,-0.75) {$\sigma$};
            \node [gt2] (ibox2a) at (-1.5,-0.75) {$\sigma$};
            \node [gt2, minimum width=1cm] (ibox3a) at (-0.5,-0.75) {Tanh};
            \node [gt3] (ibox4a) at (0.5,-0.75) {$\sigma$};
            \node [operator] (mux1a) at (-2,1.5) {$\times$};
            \node [operator] (adda) at (-0.5,1.5) {+};
            \node [operator] (mux2a) at (-0.5,0) {$\times$};
            \node [gt3] (mux3a) at (1.5,0) {$\times$};
            \node [gt3] (funca) at (1.5,0.75) {Tanh};
            \node[] (ca) at (-4,1.5) {$C_{t-2}$};
            \node[] (ha) at (-4,-1.5) {$y_{t-2}$};
            \node[] (xa) at (-2.5,-3) {$x_{t-1}$};
            \node[] (c2a) at (4,1.5) {};
            \node[] () at (3.5, 2) {$C_{t-1}$};
            \node[] (h2a) at (4,-1.5) {};
            \node[] (x2a) at (2.5,3) {$y_{t-1}$};
            \node[] () at (3.5, -2) {$y_{t-1}$};
            % \draw [->, ArrowC1] (c) -- (mux1) -- (add1) -- (c2);
            \draw [->, ArrowC1] (ca) -- (mux1a);
            \draw [->, ArrowC1] (mux1a) -- (adda);
            \draw [->, ArrowC1] (adda) -- (c2a);
            \draw [ArrowC2] (ha) -| (ibox4a);
            \draw [ArrowC1] (ha -| ibox1a)++(-0.5,0) -| (ibox1a); 
            \draw [ArrowC1] (ha -| ibox2a)++(-0.5,0) -| (ibox2a);
            \draw [ArrowC1] (ha -| ibox3a)++(-0.5,0) -| (ibox3a);
            \draw [ArrowC1] (xa) -- (xa |- ha)-| (ibox3a);
            \draw [->, ArrowC2] (ibox1a) -- (mux1a);
            \draw [->, ArrowC2] (ibox2a) |- (mux2a);
            \draw [->, ArrowC2] (ibox3a) -- (mux2a);
            \draw [->, ArrowC2] (ibox4a) |- (mux3a);
            \draw [->, ArrowC2] (mux2a) -- (adda);
            \draw [->, ArrowC1] (adda -| funca)++(-0.5,0) -| (funca);
            \draw [->, ArrowC2] (funca) -- (mux3a);
            \draw [->, ArrowC2] (mux3a) |- (h2a);
            \draw (c2a -| x2a) ++(0,-0.1) coordinate (i1a);
            \draw [-, ArrowC2] (h2a -| x2a)++(-0.5,0) -| (i1a);
            \draw [->, ArrowC2] (i1a)++(0,0.2) -- (x2a);
        },
        cellb/.pic={
            \node [cell, minimum height =4cm, minimum width=6cm] at (0,0){};
            \node [gt1] (ibox1b) at (-2,-0.75) {$\sigma$};
            \node [gt2] (ibox2b) at (-1.5,-0.75) {$\sigma$};
            \node [gt2, minimum width=1cm] (ibox3b) at (-0.5,-0.75) {Tanh};
            \node [gt3] (ibox4b) at (0.5,-0.75) {$\sigma$};
            \node [operator] (mux1b) at (-2,1.5) {$\times$};
            \node [operator] (add1b) at (-0.5,1.5) {+};
            \node [operator] (mux2b) at (-0.5,0) {$\times$};
            \node [gt3] (mux3b) at (1.5,0) {$\times$};
            \node [gt3] (func1b) at (1.5,0.75) {Tanh};
            \node[] (cb) at (-4,1.5) {};
            \node[] (hb) at (-4,-1.5) {};
            \node[] (xb) at (-2.5,-3) {$x_{t}$};
            \node[] (c2b) at (4,1.5) {};
            \node[] () at (3.5,2) {$C_{t}$};
            \node[] (h2b) at (4,-1.5) {};
            \node[] (x2b) at (2.5,3) {$y_{t}$};
            \node[] () at (3.5, -2) {$y_{t}$};
            % \draw [->, ArrowC1] (c) -- (mux1) -- (add1) -- (c2);
            \draw [->, ArrowC1] (cb) -- (mux1b);
            \draw [->, ArrowC1] (mux1b) -- (add1b);
            \draw [->, ArrowC1] (add1b) -- (c2b);
            \draw [ArrowC2] (hb) -| (ibox4b);
            \draw [ArrowC1] (hb -| ibox1b)++(-0.5,0) -| (ibox1b); 
            \draw [ArrowC1] (hb -| ibox2b)++(-0.5,0) -| (ibox2b);
            % \draw [ArrowC1] (h -| ibox3)++(-0.5,0) -| (ibox3);
            \draw [ArrowC1] (xb) -- (xb |- hb)-| (ibox3b);
            \draw [->, ArrowC2] (ibox1b) -- (mux1b);
            \draw [->, ArrowC2] (ibox2b) |- (mux2b);
            \draw [->, ArrowC2] (ibox3b) -- (mux2b);
            \draw [->, ArrowC2] (ibox4b) |- (mux3b);
            \draw [->, ArrowC2] (mux2b) -- (add1b);
            \draw [->, ArrowC1] (add1b -| func1b)++(-0.5,0) -| (func1b);
            \draw [->, ArrowC2] (func1b) -- (mux3b);
            \draw [->, ArrowC2] (mux3b) |- (h2b);
            \draw (c2b -| x2b) ++(0,-0.1) coordinate (i1b);
            \draw [-, ArrowC2] (h2b -| x2b)++(-0.5,0) -| (i1b);
            \draw [->, ArrowC2] (i1b)++(0,0.2) -- (x2b);
        },
        cellc/.pic={
            \node [cell, minimum height =4cm, minimum width=6cm] at (0,0){};
            \node [gt1] (ibox1c) at (-2,-0.75) {$\sigma$};
            \node [gt2] (ibox2c) at (-1.5,-0.75) {$\sigma$};
            \node [gt2, minimum width=1cm] (ibox3c) at (-0.5,-0.75) {Tanh};
            \node [gt3] (ibox4c) at (0.5,-0.75) {$\sigma$};
            \node [operator] (mux1c) at (-2,1.5) {$\times$};
            \node [operator] (add1c) at (-0.5,1.5) {+};
            \node [operator] (mux2c) at (-0.5,0) {$\times$};
            \node [gt3] (mux3c) at (1.5,0) {$\times$};
            \node [gt3] (func1c) at (1.5,0.75) {Tanh};
            \node[] (cc) at (-4,1.5) {};
            \node[] (hc) at (-4,-1.5) {};
            \node[] (xc) at (-2.5,-3) {$x_{t+1}$};
            \node[] (c2c) at (4,1.5) {};
            \node[] () at (3.5,2) {$C_{t+1}$};
            \node[] (h2c) at (4,-1.5) {};
            \node[] (x2c) at (2.5,3) {$y_{t+1}$};
            \node[] () at (3.5, -2) {$y_{t+1}$};
            % \draw [->, ArrowC1] (c) -- (mux1) -- (add1) -- (c2);
            \draw [->, ArrowC1] (cc) -- (mux1c);
            \draw [->, ArrowC1] (mux1c) -- (add1c);
            \draw [->, ArrowC1] (add1c) -- (c2c);
            \draw [ArrowC2] (hc) -| (ibox4c);
            \draw [ArrowC1] (hc -| ibox1c)++(-0.5,0) -| (ibox1c); 
            \draw [ArrowC1] (hc -| ibox2c)++(-0.5,0) -| (ibox2c);
            \draw [ArrowC1] (hc -| ibox3c)++(-0.5,0) -| (ibox3c);
            \draw [ArrowC1] (xc) -- (xc |- hc)-| (ibox3c);
            \draw [->, ArrowC2] (ibox1c) -- (mux1c);
            \draw [->, ArrowC2] (ibox2c) |- (mux2c);
            \draw [->, ArrowC2] (ibox3c) -- (mux2c);
            \draw [->, ArrowC2] (ibox4c) |- (mux3c);
            \draw [->, ArrowC2] (mux2c) -- (add1c);
            \draw [->, ArrowC1] (add1c -| func1c)++(-0.5,0) -| (func1c);
            \draw [->, ArrowC2] (func1c) -- (mux3c);
            \draw [->, ArrowC2] (mux3c) |- (h2c);
            \draw (c2c -| x2c) ++(0,-0.1) coordinate (i1c);
            \draw [-, ArrowC2] (h2c -| x2c)++(-0.5,0) -| (i1c);
            \draw [->, ArrowC2] (i1c)++(0,0.2) -- (x2c);
        }, 
        very thick,
    ]
    \begin{scope}[xshift=-7cm]
    \pic {cella};
    \draw[rounded corners=0.5cm, fill=green!40, opacity=0.8] (-3,-2) rectangle +(6,4);
    \node[scale=3] {$t-1$};
    \end{scope}
    \begin{scope}[xshift=0cm]
    \draw[rounded corners=0.5cm, fill=green!40, opacity=0.8] (-3,-2) rectangle +(6,4);
    \pic {cellb};
    \end{scope}
    \begin{scope}[xshift=7cm]
    \pic {cellc};
    \draw[rounded corners=0.5cm, fill=green!40, opacity=0.8] (-3,-2) rectangle +(6,4);
    \node[scale=3] {$t+1$};
    \end{scope}
    \end{tikzpicture}
    }
    }
\end{figure}

For an LSTM, the overall structure is similar to the Vanilla RNN shown in the diagram, but the hidden state computation is replaced by the LSTM's more complex gating mechanism. Using its gating mechanisms to control how information flows through time. Let us consider an input sequence $\{v_1, v_2, \dots, v_T\}$, where $v_t$ represents the input vector at time step $t$. Heres how the LSTM processes the input:

\paragraph{Step 1: Initialize Hidden and Cell States}
At the beginning of the sequence, the LSTM initializes the hidden state $h_0$ and cell state $C_0$. For our model, we use 0 matrix as $h_0$ and $C_0$:
\begin{eqnarray}
    h_0 = \mathbf{0}, \quad C_0 = \mathbf{0}
\end{eqnarray}

\paragraph{Step 2: Process Each Input Vector}
For each time step $t$, LSTM processes the current input $v_t$ along with the previous hidden state $h_{t-1}$ and cell state $C_{t-1}$ to compute:
\begin{itemize}
    \item The \textbf{forget gate} $f_t$, determine which information should be retained in memory cell $C_{t-1}$.
    \item The \textbf{input gate} $i_t$, controls how much new information from the current time step is allowed to enter the cell.
    \item The \textbf{cell candidate} $\tilde{C}_t$, represents the new information to be added to the cell state.
    \item The updated \textbf{cell state} $C_t$, which combines the retained information from $C_{t-1}$ and the new candidate $\tilde{C}_t$.
    \item The \textbf{output gate} $o_t$, control how much or what information from the cell state should be passed to the next layer or used in predictions.
    \item The updated \textbf{hidden state} $h_t$, summarizes the information at time $t$.
\end{itemize}

\paragraph{Step 3: Compute Forget Gate}
The forget gate $f_t$ is computed as:
\begin{equation}
    f_t = \sigma(W_f v_t + U_f h_{t-1} + b_f)
\end{equation}

where:
\begin{itemize}
    \item $\sigma$ is the sigmoid activation function.
    \item $W_f$ is the input to hidden weight matrix for the forget gate.
    \item $U_f$ is the hidden to hidden weight matrix for the forget gate.
    \item $b_f$ is the bias term for the forget gate.
\end{itemize}
The forget gate determines which parts of the previous cell state $C_{t-1}$ to retain.

\paragraph{Step 4: Compute Input Gate}
The input gate $i_t$ is computed as:
\begin{eqnarray}
    i_t = \sigma(W_i v_t + U_i h_{t-1} + b_i)
\end{eqnarray}

where:
\begin{itemize}
    \item $W_i$, $U_i$, and $b_i$ are the weight matrices and bias for the input gate.
\end{itemize}
The input gate determines how much new information to add to the cell state.

\paragraph{Step 5: Compute Cell Candidate}
The cell candidate $\tilde{C}_t$ is computed as:
\begin{equation}
    \tilde{C}_t = \tanh(W_c v_t + U_c h_{t-1} + b_c)
\end{equation}

where:
\begin{itemize}
    \item $\tanh$ is the hyperbolic tangent activation function.
    \item $W_c$, $U_c$, and $b_c$ are the weight matrices and bias for the cell candidate.
\end{itemize}
The cell candidate represents potential new information to add to the cell state.

\paragraph{Step 6: Update Cell State}
The cell state $C_t$ is updated as:
\begin{equation}
    C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
\end{equation}

where $\odot$ represents element-wise multiplication. The updated cell state combines retained information from $C_{t-1}$ (via $f_t$) and new information from $\tilde{C}_t$ (via $i_t$).

\paragraph{Step 7: Compute Output Gate}
The output gate $o_t$ is computed as:
\begin{equation}
    o_t = \sigma(W_o v_t + U_o h_{t-1} + b_o)
\end{equation}

where $W_o$, $U_o$, and $b_o$ are the weight matrices and bias for the output gate.

\paragraph{Step 8: Compute Hidden State}
The hidden state $h_t$ is computed as:
\begin{equation}
    h_t = o_t \odot \tanh(C_t)
\end{equation}

The hidden state summarizes the information at time $t$ and is passed to the next time step.

\paragraph{Step 9: Use Final Hidden State for Output}
After processing all time steps, the final hidden state $h_T$ is used as the input to a fully connected layer for classification:
\begin{equation}
    \hat{y} = f(W_{ho}^T h_T + b_y)    
\end{equation}

where $W_{ho}$ and $b_y$ are the weight matrix and bias for the output layer, and $f$ is the activation function. Then we calculate the loss function by the last output.
\begin{equation}
    Loss = \mathcal{L}(\hat{y}, y)
\end{equation}


\subsubsection{Model 3 (Gated recurrent unit)}
For the GRU model, I use 1 layer. This architecture is similar with LSTM but reduce the among of parameters. Let us consider an input sequence $\{v_1, v_2, \dots, v_T\}$, where $v_t$ represents the input vector at time step $t$. Here's how the GRU processes the input:

\begin{figure}[tbh!]
    \centering   
    \begin{tikzpicture}[
        font=\sf \tiny,
        >=LaTeX,
        % Styles
        cell/.style={% For the main box
            rectangle, 
            rounded corners=5mm, 
            draw,
            very thick,
            },
        cell2/.style={% For the main box
        rectangle, 
        rounded corners=2mm, 
        draw,
        thin,
        },
        operator/.style={% For operators like +  and  x
            circle,
            draw,
            inner sep=-0.5pt,
            minimum height =.2cm,
            },
        function/.style={% For functions
            ellipse,
            draw,
            inner sep=1pt
            },
        ct/.style={% For external inputs and outputs
            circle,
            draw,
            line width = .75pt,
            minimum width=1cm,
            inner sep=0pt,
            },
        gt/.style={% For internal inputs
            rectangle,
            draw,
            minimum width=3mm,
            minimum height=3mm,
            inner sep=1pt
            },
        mylabel/.style={% something new that I have learned
            font=\tiny\sffamily
            },
        ArrowC1/.style={% Arrows with rounded corners
            rounded corners=.25cm,
            thick,
            },
        ArrowC2/.style={% Arrows with big rounded corners
            rounded corners=.5cm,
            thick,
            },
        ]
    
    % Nodes    
        % Draw the cell boundary: 
        \node [cell, minimum height=3.5cm, minimum width=3.5cm] at (0,0){} ;
        \node [dashed, red, cell2, minimum height=3cm, minimum width=0.6cm] at (0.05,0.175){} ;
        \node [dashed, blue, cell2, minimum height=3cm, minimum width=0.85cm] at (-0.75,0.15){} ;
    
        % Draw inputs named ibox#
        \node [gt] (sigma1) at (-0.5,-0.75) {$\sigma$};
        \node [gt] (sigma2) at (0,-0.75) {$\sigma$};
        \node [gt, minimum width=0.5cm] (tanh) at (1,-0.75) {Tanh};
        \node [gt] (oneminus) at (0,0.25) {$1-$};
    
       % Draw operators named mux# , add# and func#
        \node [operator] (mux1) at (-1,0.5) {$\times$};
        \node [operator] (add1) at (1,1.5) {+};
        \node [operator] (mux2) at (1,0) {$\times$};
        \node [operator] (mux3) at (0,1.5) {$\times$};
    
        % Draw External inputs named as basis h,x
        \node[ct, label={[mylabel]Previous hidden state}, align=center, inner sep=0pt, minimum size=4mm] (h) at (-2.5,1.5) {$h^{t-1}$};
        \node[ct, label={[mylabel]left:Input}, align=center, inner sep=0pt, minimum size=4mm] (x) at (-1,-2.5) {$x^{t}$};
    
        % Draw External outputs named as basis h2,y2
        \node[ct, label={[mylabel]Current hidden state}, align=center, inner sep=0pt, minimum size=4mm] (h2) at (2.25,1.5) {$h^{t}$};
        \node[ct, label={[mylabel]}, align=center, inner sep=0pt, minimum size=4mm] (y2) [below of= h2] {$y^{t}$};
    
    % Start connecting all.
        % Intersections and displacements are used. 
        % Drawing arrows    
        \draw [ArrowC1] (h) -- (mux3) -- (add1) -- (h2);
        \draw [ArrowC1] (add1) coordinate[auto] -|(y2.west);
    
        % Inputs
        \draw [ArrowC1] (x) -- coordinate[auto] (link1) (mux1); 
        \draw [ArrowC1] (x -| sigma2)++(-1,1.25) -| (sigma2);
        \draw [ArrowC1] (x -| tanh)++(-2,1) -| (tanh);
        \draw [ArrowC1] (x.north)++(0,1.04) -| (sigma1.south); 
        \draw [->, ArrowC1] (mux1) ++(0,0.1) |- (mux3) ;
        \draw [->,ArrowC1] (h.east)++(0.6,0) |- (mux1.west);
        \draw [->, ArrowC1] (sigma1.north) |- coordinate[auto]  node[pos=0.5] {$r_t$} (mux1.east);
    
        % Internal
        \draw [->, ArrowC2] (sigma2) |- (mux2);
        \draw [->, ArrowC2] (tanh) -- node[pos=0.4, right] {$\widetilde{h}_t$}(mux2);
        \draw [->, ArrowC2] (oneminus) -- coordinate[auto] node[pos=0.75, right] {$u_t$} (mux3);
        \draw [->, ArrowC2] (sigma2)++(0,0.25) -| node[pos=0.4, right] {$z_t$} (oneminus);
        \draw [->, ArrowC2] (mux2) -- (add1);
        % \draw [ArrowC2] (h.east)++(0.6,0) |- ($(x.north)!0.775!(link1)$);  
        % \draw[->, ArrowC2] (h.east)++(0.25,0) to [out=0,in=100, looseness=0.85] ($(x.north)!0.2!(h.east)$);
    
    \end{tikzpicture}
\end{figure} 

\paragraph{Step 1: Initialize Hidden State}
At the beginning of the sequence, the GRU initializes the hidden state $h_0$ to zero:
\begin{equation}
    h_0 = \mathbf{0}
\end{equation}

\paragraph{Step 2: Process Each Input Vector}
For each time step $t$, the GRU processes the current input $v_t$ along with the previous hidden state $h_{t-1}$ to compute:
\begin{itemize}
    \item The \textbf{reset gate} $r_t$, which determines how much of the previous hidden state $h_{t-1}$ to forget.
    \item The \textbf{update gate} $z_t$, which determines how much of the previous hidden state $h_{t-1}$ to retain.
    \item The \textbf{candidate hidden state} $\tilde{h}_t$, which represents the new information to be combined with the previous hidden state.
    \item The updated \textbf{hidden state} $h_t$, which summarizes the information at time $t$.
\end{itemize}

\paragraph{Step 3: Compute Reset Gate}
The reset gate $r_t$ is computed as:
\begin{equation}
    r_t = \sigma(W_r v_t + U_r h_{t-1} + b_r)
\end{equation}

where:
\begin{itemize}
    \item $\sigma$ is the sigmoid activation function.
    \item $W_r$ is the input to hidden weight matrix for the reset gate.
    \item $U_r$ is the hidden to hidden weight matrix for the reset gate.
    \item $b_r$ is the bias term for the reset gate.
\end{itemize}
The reset gate determines how much of the previous hidden state $h_{t-1}$ to use when computing the candidate hidden state.

\paragraph{Step 4: Compute Update Gate}
The update gate $z_t$ is computed as:
\begin{equation}
    z_t = \sigma(W_z v_t + U_z h_{t-1} + b_z)
\end{equation}

where:
\begin{itemize}
    \item $W_z$ is the input to hidden weight matrix for the update gate.
    \item $U_z$ is the hidden to hidden weight matrix for the update gate.
    \item $b_z$ is the bias term for the update gate.
\end{itemize}
The update gate determines how much of the previous hidden state $h_{t-1}$ to retain and how much of the new candidate hidden state to use.

\paragraph{Step 5: Compute Candidate Hidden State}
The candidate hidden state $\tilde{h}_t$ is computed as:
\begin{equation}
    \tilde{h}_t = \tanh(W_h v_t + U_h (r_t \odot h_{t-1}) + b_h)    
\end{equation}

where:
\begin{itemize}
    \item $\tanh$ is the hyperbolic tangent activation function.
    \item $r_t \odot h_{t-1}$: The reset gate modulates the contribution of the previous hidden state $h_{t-1}$.
    \item $W_h$, $U_h$, and $b_h$ are the weight matrices and bias for the candidate hidden state.
\end{itemize}

\paragraph{Step 6: Update Hidden State}
The updated hidden state $h_t$ is computed as:
\begin{equation}
    h_t = z_t \odot h_{t-1} + (1 - z_t) \odot \tilde{h}_t    
\end{equation}

where:
\begin{itemize}
    \item $z_t$: The update gate decides how much of the previous hidden state $h_{t-1}$ to retain.
    \item $(1 - z_t)$: The complement of the update gate decides how much of the candidate hidden state $\tilde{h}_t$ to use.
\end{itemize}

\paragraph{Step 7: Use Final Hidden State for Output}
After processing all time steps, the final hidden state $h_T$ is used as the input to a fully connected layer for classification:
\begin{equation}
    \hat{y} = f(W_{ho}^T h_T + b_y)    
\end{equation}

where:
\begin{itemize}
    \item $W_{ho}$ and $b_y$ are the weight matrix and bias for the output layer.
    \item $f$ is the activation function (e.g., sigmoid for binary classification).
\end{itemize}

Then we calculate the loss function by the last output.
\begin{equation}
    Loss = \mathcal{L}(\hat{y}, y)
\end{equation}

\subsubsection{Model 4 (Vanilla RNN with 2 layer)}
Same setting with model 1, but I stack 2 RNN block. In the other word, this is deep recurrent neural networks. 
\subsubsection{Model 5 (GRU with 2 layer)}
Same setting with model 1, but I stack 2 GRU block. In the other word, this is deep recurrent neural networks. 
\subsubsection{Model 6 (LSTM RNN with 2 layer)}
Same setting with model 1, but I stack 2 LSTM block. In the other word, this is deep recurrent neural networks. 

\subsubsection{Model 7 (distilbert-base-uncased-finetuned-sst-2-english)}


\newpage
\subsection{Training, Hyperparameter Tuning, and Evaluation}
In my experiments, I implemented the training loop using PyTorch. The following details describe the key components of my training code, which are designed to ensure robust model optimization and fair evaluation:
    
\subsubsection{Model Hyperparameters and Setup}
The following hyperparameters were chosen based on preliminary experiments and standard practices for sentiment classification tasks:
\begin{itemize}
    \item \textbf{Architecture Hyperparameters:}
    \begin{itemize}
        \item Number of layers: \texttt{no\_layers = 1}
        \item Vocabulary size: \(\mathtt{vocab\_size = len(vocab) + 1}\) (the extra one is for unknown word)
        \item Embedding dimension: \texttt{embedding\_dim = 100}
        \item Hidden dimension: \texttt{hidden\_dim = 50}
        \item Output dimension: \texttt{output\_dim = 1}
        \item Dropout probability: \texttt{drop\_prob = 0.5}
        \item Batch size: \texttt{batch\_size = 50}
    \end{itemize}
    \item \textbf{Optimization Hyperparameters:}
    \begin{itemize}
        \item Learning rate: \texttt{lr = 0.001}
        \item Loss Function: Binary Cross Entropy Loss
        \item Optimizer: Adam with predefined learning rate.
    \end{itemize}
\end{itemize}
In order to keep the fair experiment. I train 3 RNN model with the same hyperparameters

\subsubsection{Training Loop Overview}
\begin{itemize}
    \item \textbf{Mini-Batch Training with Hidden State Initialization:}  
    At the start of each epoch, the model is set to training mode using \texttt{model.train()}. For each mini-batch from the training data:
    \begin{enumerate}
        \item The models hidden state is initialized with zero matrix.
        \item Before the forward pass, gradients are zeroed via \texttt{model.zero\_grad()} and \texttt{optimizer.zero\_grad()}.
        \item The forward pass is executed, producing outputs and an updated hidden state.
        \item The hidden state is detached from the computation graph using \texttt{h.detach()} to avoid backpropagating through all previous batches.
        \item The loss is computed using binary cross entropy.
        \item Backpropagation is performed via \texttt{loss.backward()}.
        \item The gradients are clipped using \texttt{nn.utils.clip\_grad\_norm\_} with the specified clip value to mitigate exploding gradients.
        \item Finally, the optimizer updates the model parameters using \texttt{optimizer.step()}.
    \end{enumerate}
    
    \item \textbf{Validation Stage:}  
    After each epoch, the model is switched to evaluation mode using \texttt{model.eval()} and evaluated on the validation set. Similar to training, the hidden state is updated parameters and the model will use that parameters to make the prediction. And the accuracy is computed and recorded.
    
    \item \textbf{Performance Monitoring and Checkpointing:}  
    At the end of each epoch, the mean training and validation losses, as well as accuracy, are calculated. The validation loss is compared against the best loss seen so far. If the current validation loss is lower than the lowerest one, then the model's state dictionary is saved as the best model in .pth format. 
\end{itemize}

\subsubsection{Pseudocode Description}
The following Python pseudocode summarizes the training procedure implemented in my experiment:

\begin{lstlisting}[language=Python, caption={Training Loop Pseudocode}]
clip = 5
epochs = 5 
valid_loss_min = np.inf
epoch_tr_loss, epoch_vl_loss = [], []
epoch_tr_acc, epoch_vl_acc = [], []

for epoch in range(epochs):
    train_losses = []
    train_acc = 0.0
    model.train()
    # Initialize the hidden state for the first mini-batch
    h = model.init_hidden(batch_size)
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        model.zero_grad()
        optimizer.zero_grad()
        
        output, h = model(inputs, h)
        h = h.detach()
        
        loss = criterion(output, labels.float())
        loss.backward()
        train_losses.append(loss.item())
        
        accuracy = acc(output, labels)
        train_acc += accuracy
        
        nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()
    
    val_h = model.init_hidden(batch_size)
    val_losses = []
    val_acc = 0.0
    model.eval()
    for inputs, labels in valid_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        output, val_h = model(inputs, val_h)
        val_loss = criterion(output.squeeze(), labels.float())
        val_losses.append(val_loss.item())
        
        accuracy = acc(output, labels)
        val_acc += accuracy
    
    epoch_train_loss = np.mean(train_losses)
    epoch_val_loss = np.mean(val_losses)
    epoch_train_acc = train_acc / len(train_loader.dataset)
    epoch_val_acc = val_acc / len(valid_loader.dataset)
    
    epoch_tr_loss.append(epoch_train_loss)
    epoch_vl_loss.append(epoch_val_loss)
    epoch_tr_acc.append(epoch_train_acc)
    epoch_vl_acc.append(epoch_val_acc)
    
    print(f'Epoch {epoch+1}')
    print(f'train_loss : {epoch_train_loss}  val_loss : {epoch_val_loss}')
    print(f'train_accuracy : {epoch_train_acc*100}  val_accuracy : {epoch_val_acc*100}')
    
    if epoch_val_loss <= valid_loss_min:
        torch.save(model.state_dict(), './working/state_dict.pt')
        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'
              .format(valid_loss_min, epoch_val_loss))
        valid_loss_min = epoch_val_loss
    print(25 * '==')
\end{lstlisting}
\newpage

% ######################## Results and Analysis ######################################
\section{Results and Analysis}
\subsection{Results}
In this section, I report the classification accuracy of three model on the testing set. And I will show the performance of RNN, GRU and LSTM on IMDB dataset respectively. 
\subsubsection{Vanilla RNN (1 layer)}
For the figure 2, RNN with 1 layer performance on IMDB dataset. Here is the deeply interpretation. For the left hand side is the accuracy. We can see that the training accuracy increasing steadily from 50\% to roughly 65\%. And the validation accuracy shows a more erratic behavior. It starts from 50\% and increases to a peak around epoch 3, drops slightly at epoch 4 and the rises to 65\% by the end of the training. 
\\[1ex]
For the training loss, it decreases steadily over 5 epochs from 0.7 to 0.6, showing that the model is learning. On the contrary, the validation loss decreases initially and stabilizes around epoch 4 and then rises sharply in later epochs. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{../Pic/rnn1.png} 
    \caption{RNN with 1 layer performance on IMDB dataset}
\end{figure}
\subsubsection{Vanilla RNN (2 layer)}
The training accuracy improves over the eopchs, increasing steadily from 50\% to 75\%. For the validation accuracy also improves, although at a slower pace than training accuracy. Starts at around 50\% and reaching approximately 67\% by epoch 5.
\\[1ex]
Then for the tarining loss, it decreases steadily from 0.7 to 0.54, showing that the model is learning effectively and optimizing the training objective. Next for the validation loss, decreases initially, but plateaus and slightly increases after epoch 4. 
\newpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{../Pic/rnn2.png} 
    \caption{RNN with 2 layer performance on IMDB dataset}
\end{figure}
\subsubsection{GRU (1 layer)}
For the figure 4, GRU with 1 layer, we can see that the training accuracy improves steadily and reaching around 95\%, but the validation accuracy remain the same value after epoch 2. Which mean that the model did not learning any new information from the training dataset. 
\\[1ex]
Then for the training loss, the performance is relatively good and drop from 0.7 to 0.15. But for the validation loss, it drop from 0.55 to 0.4 and then rises to 0.45. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{../Pic/gru1.png} 
    \caption{GRU with 1 layer performance on IMDB dataset}
\end{figure}
\subsubsection{GRU (2 layer)}
And next is the performance of GRU with 1 layer, we can see that the training accuracy improves significantly, starting around 60\% and reaching over 95\% by the end of training. This indicates that the GRU model is fitting the training data very well. On the contrast, the validation accuracy starts at around 80\% and improves slightly in the first 2 epochs but then remain on the same value around 85\% for the rest of the training. The widening gap between training and validation accuracy after the $2^{nd}$ epoch suggests overfitting. 
\\[1ex]
Then for the training loss and the validation loss for the GRU model. The training loss decreases steadily and starting at over 0.6 and dropping to around 0.1 by the $5^{th}$ epoch. The gap between training and validation accuracy after the second epoch suggests overfitting.
\\[1ex]
For the loss, the training loss decreases steadily, starting at over 0.6 and dropping to around 0.1 by the $5^{th}$ epoch. For the validation loss decreases initially and reaching the lowest and then the loss raise over 0.5. Which indicate that overfitting.
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{../Pic/gru2.png} 
    \caption{GRU with 2 layer performance on IMDB dataset}
\end{figure}

\subsubsection{LSTM (1 layer)}
And then for the training accuracy of LSTM with 1 layer, the train and validation accuracy increase steadily, for the training accuracy increase from 55\% to 90\%, for the validation loss is increase from 63\% to 83\%. For the training loss also decrease steadily, from 0.7 to 0.3. And the validation loss decrease from 0.7 to 0.43.
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{../Pic/lstm1performance.png} 
    \caption{LSTM with 1 layer performance on IMDB dataset}
\end{figure}
\subsubsection{LSTM (2 layer)}
Next is the performance of LSTM with 1 layer, the training accuracy improves steadily over the epochs and starting at around 55\% and reaching about 90\% at the end of the training. And for the validation accuracy starting at approximately 65\% and reaching about 82.5\%. And the loss of the training dataset decreases steadily from about 0.7 to around 0.2. And the validation loss show that the model did learn something from the data. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{../Pic/lstm2performance.png}
    \caption{LSTM with 2 layer performance on IMDB dataset}
\end{figure}

\newpage
\section{Analysis}
\subsection{Learning Behavior}
All models show the effective learning during the initial epochs, roughtly 55\%, as both training and validation accuracy improve, and loss decrease. But the common problem is overfitting occure in 3 models. The possible reasons is dataset. Since every sentence in dataset was truncated. I just use the first 100 token to training the model. So the existing tokens might not be fully represent the sentence. The second reason is the embedding layers, I used the built-in function in Pytorch to create the embedding layer by randomise the matrix. 
\\[1ex]
Among the 3 models, LSTM and GRU have the better performance compare with vanilla RNN. The reason of GRU and LSTM perform better than RNN is the vanilla RNN struggles with long term dependencies because of the vanishing gradient problem when processing long sequences. In this experience is 100 time step. And this limits its ability to learn the meaningful patterns from data. On the contrast, GRU and LSTM use gate mechanisms to retain the relevant information and discard irrelevant information, allowing them to capture the long term dependencies more effectively.
\\[1ex]
And for the overfitting problem, we can see that overfitting problem occur in three models. For the vanilla RNN, since this is the basic and simple architecture to handle sequential data, it lacks mechanisms to retain or forget information selectively. As a result, it relying on the memorizing the training data rather than learning the meaningful patterns. This causes it to overfit. And for the GRU and LSTM, although gating mechanisms provide a more sophisticated architecture to learning from data, they aggressively optimizes the training loss, which can result in overfitting when the model continues to learn unnecessary information from the dataset. 
\\[1ex] 
The above result and analyse shown that GRU and LSTM perform better than vanilla RNN, which show that the graident vanishing and graident exploding problem were improved by the gating mechanism. And there is so many way will affect the experiment result for example, the embedding layer, tokenization.
\newpage

% ######################## Results and Analysis ######################################








% ######################## Reference ######################################
\printbibliography
% ######################## Reference ######################################

\end{document}