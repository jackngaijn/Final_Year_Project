\contentsline {section}{\numberline {1}Introduction}{3}{section.1}%
\contentsline {section}{\numberline {2}Background and Literature Review}{3}{section.2}%
\contentsline {subsection}{\numberline {2.1}Evolution of Deep Learning and Recurrent Neural Networks}{3}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Literature Review}{4}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Backpropagation Through Time}{4}{subsubsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2}Activation Function}{8}{subsubsection.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.3}Gradient vanishing and gradient exploring}{10}{subsubsection.2.2.3}%
\contentsline {subsubsection}{\numberline {2.2.4}Long short-term memory (LSTM)}{10}{subsubsection.2.2.4}%
\contentsline {subsubsection}{\numberline {2.2.5}Gated Recurrent Unit (GRU)}{13}{subsubsection.2.2.5}%
\contentsline {subsubsection}{\numberline {2.2.6}Deep recurrent neural networks (DRNNs)}{15}{subsubsection.2.2.6}%
\contentsline {subsubsection}{\numberline {2.2.7}RNN encoder decoder model}{16}{subsubsection.2.2.7}%
\contentsline {subsubsection}{\numberline {2.2.8}Attention Mechanism}{17}{subsubsection.2.2.8}%
\contentsline {subsubsection}{\numberline {2.2.9}Hidden Markov Model}{18}{subsubsection.2.2.9}%
\contentsline {subsubsection}{\numberline {2.2.10}Word representation}{20}{subsubsection.2.2.10}%
\contentsline {section}{\numberline {3}Project Goals and Objectives}{21}{section.3}%
\contentsline {subsection}{\numberline {3.1}Project Goals}{21}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Objectives}{21}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}Implement Existing RNN Architectures}{21}{subsubsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.2}Apply the Models on a Benchmark NLP Dataset}{21}{subsubsection.3.2.2}%
\contentsline {subsubsection}{\numberline {3.2.3}Compare Model Performances}{22}{subsubsection.3.2.3}%
\contentsline {subsubsection}{\numberline {3.2.4}Analyze the Impact of Architectural Differences}{22}{subsubsection.3.2.4}%
\contentsline {section}{\numberline {4}Research Plan / Methodology}{22}{section.4}%
\contentsline {subsection}{\numberline {4.1}Literature Review}{22}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Data Collection and Preprocessing}{22}{subsection.4.2}%
\contentsline {subsubsection}{\numberline {4.2.1}Data Collection}{22}{subsubsection.4.2.1}%
\contentsline {subsubsection}{\numberline {4.2.2}Data Preprocessing}{22}{subsubsection.4.2.2}%
\contentsline {subsection}{\numberline {4.3}Model Architecture}{23}{subsection.4.3}%
\contentsline {subsubsection}{\numberline {4.3.1}Model 1 (Vanilla RNN)}{24}{subsubsection.4.3.1}%
\contentsline {subsubsection}{\numberline {4.3.2}Model 2 (Long short-term memory)}{25}{subsubsection.4.3.2}%
\contentsline {paragraph}{Step 1: Initialize Hidden and Cell States}{25}{section*.2}%
\contentsline {paragraph}{Step 2: Process Each Input Vector}{25}{section*.3}%
\contentsline {paragraph}{Step 3: Compute Forget Gate}{26}{section*.4}%
\contentsline {paragraph}{Step 4: Compute Input Gate}{26}{section*.5}%
\contentsline {paragraph}{Step 5: Compute Cell Candidate}{26}{section*.6}%
\contentsline {paragraph}{Step 6: Update Cell State}{26}{section*.7}%
\contentsline {paragraph}{Step 7: Compute Output Gate}{26}{section*.8}%
\contentsline {paragraph}{Step 8: Compute Hidden State}{27}{section*.9}%
\contentsline {paragraph}{Step 9: Use Final Hidden State for Output}{27}{section*.10}%
\contentsline {subsubsection}{\numberline {4.3.3}Model 3 (Gated recurrent unit)}{27}{subsubsection.4.3.3}%
\contentsline {paragraph}{Step 1: Initialize Hidden State}{27}{section*.11}%
\contentsline {paragraph}{Step 2: Process Each Input Vector}{27}{section*.12}%
\contentsline {paragraph}{Step 3: Compute Reset Gate}{28}{section*.13}%
\contentsline {paragraph}{Step 4: Compute Update Gate}{28}{section*.14}%
\contentsline {paragraph}{Step 5: Compute Candidate Hidden State}{28}{section*.15}%
\contentsline {paragraph}{Step 6: Update Hidden State}{28}{section*.16}%
\contentsline {paragraph}{Step 7: Use Final Hidden State for Output}{29}{section*.17}%
\contentsline {subsubsection}{\numberline {4.3.4}Model 4 (Vanilla RNN with 2 layer)}{29}{subsubsection.4.3.4}%
\contentsline {subsubsection}{\numberline {4.3.5}Model 5 (Gated recurrent unit with 2 layer)}{29}{subsubsection.4.3.5}%
\contentsline {subsubsection}{\numberline {4.3.6}Model 6 (Long short-term memory with 2 layer)}{29}{subsubsection.4.3.6}%
\contentsline {subsubsection}{\numberline {4.3.7}Model 7 (RNN Encoder-Decoder)}{29}{subsubsection.4.3.7}%
\contentsline {subsubsection}{\numberline {4.3.8}Model 8 (RNN Encoder-Decoder with attention mechanism)}{29}{subsubsection.4.3.8}%
\contentsline {subsection}{\numberline {4.4}Training, Hyperparameter Tuning, and Evaluation}{30}{subsection.4.4}%
\contentsline {subsubsection}{\numberline {4.4.1}Model Hyperparameters and Setup}{30}{subsubsection.4.4.1}%
\contentsline {subsubsection}{\numberline {4.4.2}Training Loop Overview}{31}{subsubsection.4.4.2}%
\contentsline {subsubsection}{\numberline {4.4.3}Pseudocode Description}{32}{subsubsection.4.4.3}%
\contentsline {section}{\numberline {5}Results and Analysis}{34}{section.5}%
\contentsline {subsection}{\numberline {5.1}Results}{34}{subsection.5.1}%
\contentsline {subsubsection}{\numberline {5.1.1}Model 1 (Vanilla RNN)}{34}{subsubsection.5.1.1}%
\contentsline {subsubsection}{\numberline {5.1.2}Model 4 (Vanilla RNN with 2 layer)}{34}{subsubsection.5.1.2}%
\contentsline {subsubsection}{\numberline {5.1.3}Model 3 (Gated recurrent unit)}{35}{subsubsection.5.1.3}%
\contentsline {subsubsection}{\numberline {5.1.4}Model 5 (Gated recurrent unit with 2 layer)}{35}{subsubsection.5.1.4}%
\contentsline {subsubsection}{\numberline {5.1.5}Model 2 (Long short-term memory)}{36}{subsubsection.5.1.5}%
\contentsline {subsubsection}{\numberline {5.1.6}Model 6 (Long short-term memory with 2 layer)}{36}{subsubsection.5.1.6}%
\contentsline {subsubsection}{\numberline {5.1.7}Model 7 (RNN encoder-decoder)}{38}{subsubsection.5.1.7}%
\contentsline {subsubsection}{\numberline {5.1.8}Model 8 (RNN encoder-decoder with attention mechanism)}{39}{subsubsection.5.1.8}%
\contentsline {section}{\numberline {6}Analysis}{40}{section.6}%
\contentsline {subsection}{\numberline {6.1}Learning Behavior}{40}{subsection.6.1}%
\contentsline {subsubsection}{\numberline {6.1.1}Vanilla RNN (Red line)}{41}{subsubsection.6.1.1}%
\contentsline {subsubsection}{\numberline {6.1.2}LSTM (Blue line)}{41}{subsubsection.6.1.2}%
\contentsline {subsubsection}{\numberline {6.1.3}GRU (Green line)}{42}{subsubsection.6.1.3}%
\contentsline {section}{\numberline {7}Discussion and Conclusion}{43}{section.7}%
\contentsline {subsection}{\numberline {7.1}Discussion}{43}{subsection.7.1}%
\contentsline {subsection}{\numberline {7.2}Conclusion}{43}{subsection.7.2}%
