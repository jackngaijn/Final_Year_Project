{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_csv = './data/IMDB Dataset.csv'\n",
    "df = pd.read_csv(base_csv)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: torch.Size([30522, 768])\n",
      "{'input_ids': tensor([[ 101, 2028, 1997,  ...,    0,    0,    0],\n",
      "        [ 101, 1037, 6919,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 2245,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 1045, 2572,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 1005,  ...,    0,    0,    0],\n",
      "        [ 101, 2053, 2028,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "# Load pretrained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Load pretrained model\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Access the embedding matrix\n",
    "embedding_matrix = model.embeddings.word_embeddings.weight\n",
    "print(\"Embedding matrix shape:\", embedding_matrix.shape)  # Shape: (vocab_size, embedding_dim)\n",
    "\n",
    "# Tokenize the text column\n",
    "def tokenize_texts(texts, tokenizer):\n",
    "    return tokenizer(\n",
    "        list(texts),  # Pass a list of strings\n",
    "        padding=\"max_length\",  # Pad to max_length\n",
    "        truncation=True,       # Truncate to max_length\n",
    "        return_tensors=\"pt\"    # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "# Tokenize the `text` column\n",
    "tokenized_data = tokenize_texts(df[\"review\"], tokenizer)\n",
    "label = df['sentiment'].apply(lambda x: 1 if x =='positive' else 0)\n",
    "\n",
    "# Print tokenized outputs\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 101, 2028, 1997,  ...,    0,    0,    0],\n",
      "        [ 101, 1037, 6919,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 2245,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 1045, 2572,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 1005,  ...,    0,    0,    0],\n",
      "        [ 101, 2053, 2028,  ...,    0,    0,    0]])\n",
      "Attention Mask: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenized_data[\"input_ids\"]\n",
    "attention_mask = tokenized_data[\"attention_mask\"]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_input_ids, test_input_ids, train_attention_mask, test_attention_mask, train_labels, test_labels = train_test_split(\n",
    "    input_ids, attention_mask, label, test_size=0.2, random_state=42  # 20% for testing\n",
    ")\n",
    "\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention Mask:\", attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(train_input_ids, torch.from_numpy(train_labels.values))\n",
    "valid_data = TensorDataset(test_input_ids, torch.from_numpy(test_labels.values))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 128\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.rnn = torch.nn.LSTM(embedding_dim,\n",
    "                                 hidden_dim)        \n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, text):\n",
    "        # text dim: [sentence length, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        # embedded dim: [sentence length, batch size, embedding dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        # output dim: [sentence length, batch size, hidden dim]\n",
    "        # hidden dim: [1, batch size, hidden dim]\n",
    "\n",
    "        hidden.squeeze_(0)\n",
    "        # hidden dim: [batch size, hidden dim]\n",
    "        \n",
    "        output = self.fc(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general Settings\n",
    "\n",
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "VOCABULARY_SIZE = tokenizer.vocab_size\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 768\n",
    "HIDDEN_DIM = 768\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = RNN(input_dim=VOCABULARY_SIZE,\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            output_dim=NUM_CLASSES # could use 1 for binary classification\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            features = torch.transpose(features, 0, 1)\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            logits = model(features)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 53.24%\n",
      "valid accuracy: 50.85%\n",
      "Time elapsed: 0.53 min\n",
      "training accuracy: 70.74%\n",
      "valid accuracy: 66.61%\n",
      "Time elapsed: 1.05 min\n",
      "training accuracy: 78.34%\n",
      "valid accuracy: 72.74%\n",
      "Time elapsed: 1.57 min\n",
      "training accuracy: 83.69%\n",
      "valid accuracy: 78.09%\n",
      "Time elapsed: 2.09 min\n",
      "training accuracy: 85.56%\n",
      "valid accuracy: 79.86%\n",
      "Time elapsed: 2.61 min\n",
      "Total Training Time: 2.61 min\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)  \n",
    "        inputs = torch.transpose(inputs, 0, 1)\n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(inputs)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        # if not batch_idx % 50:\n",
    "        #     print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "        #            f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "        #            f'Loss: {loss:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "        \n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "# print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
